# 开发环境搭建

前几天因为配置了新机器，所以个人的开发测试环境需要重新搭建一下，其实需要的内容不多

hadoop+hive+kafka+hbase就可以了，hbase可以看需求。不需要部署Spark，因为是个人环境，代码直接在IDE里面运行即刻，但是Spark上面提到的几个就很必要了。另外我打算使用虚拟机的，所以就不装yarn了。

http://dblab.xmu.edu.cn/blog/install-hadoop-in-centos/


## hadoop - 需要配置好JDK

- http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

- 安装SSH、配置SSH无密码登陆

cd ~/.ssh/                     # 若没有该目录，请先执行一次ssh localhost
ssh-keygen -t rsa              # 会有提示，都按回车就可以
cat id_rsa.pub >> authorized_keys  # 加入授权
chmod 600 ./authorized_keys    # 修改文件权限

- 下载hadoop，解压到目录

修改环境变量 gedit ~/.bashrc
```bash
# Hadoop Environment Variables
export HADOOP_HOME=解压目录
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
```

  - 更改配置（伪集群模式）
  - core-site.xml
  ```xml
  <configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
  ```
  - hdfs-site.xml
  ```xml
  <configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
  ```

- NameNode 格式化（注意自己把bin加到path下面，以后好执行命令）

hdfs namenode -format

- jps查看一下启动的内容，http://localhost:50070/ 查看运行状况

## Hive
- http://dblab.xmu.edu.cn/blog/hive-in-practice/

- mysql 创建 hive 库（空的）和具有相应权限的hive用户
- conf里面配置 hive-site.xml

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/usr/local/hive/warehouse</value>   
    <description>！！这个文件夹需要自己手动创建一下的 location of default database for the warehouse</description>
  </property>
<property>
    <name>javax.jdo.option.ConnectionURL</name>
   <value>jdbc:mysql://localhost:3306/hive</value>                           
    <description>JDBC connect string for a JDBC metastore</description>
  </property>
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>
<property> 
   <name>javax.jdo.option.ConnectionPassword </name> 
   <value>hive</value> 
</property> 
 <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>Username to use against metastore database</description>
   </property>
</configuration>
```

- mysql-connector-java 放到 lib目录下（新版本似乎已经内置集成mysql功能了，不需要）
  - Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
- 配置一下 ~/.bashrc
```bash
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
```

- 启动=>  hive

## kafka 参考 02.KafkaBasic