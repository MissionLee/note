# 总结并且上代码

- 1.配置文件就放在resource下，统领也是这么说的，一般都这么干。
- 2.sourceRoot的事情，另外有一个总结了，这里不多说
- 3.有两个主要的 方法，一种叫做 原始DAO层开发方式，一种叫做Mapper代理开发方式

这里先上一份 主配置文件

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE configuration
        PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
    <!-- search mybatis.xml for more details  -->
    <!-- 这个 dbconfig.perperties 里面就是下面 jdbc需要的 几个参数，mybatis可以直接读取然后放到Config里面 -->
    <properties resource="dbconfig.properties">
    </properties>
    <!-- 这里有两个常用，并且重要的 配置！！！1 -->
        <!-- 全局配置 -->
    <settings>
        <!--允许 JDBC 支持自动生成主键-->
        <setting name="useGeneratedKeys" value="false"/>
       <!--是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 -->
        <setting name="mapUnderscoreToCamelCase" value="true"/>
    </settings>
    <typeAliases>
        <package name="com.lms.etl.spark.pojo"/>
        <package name="com.lms.etl.spark.mapper"/>
    </typeAliases>
    <environments default="development">
        <environment id="development">
            <transactionManager type="JDBC"/>
            <dataSource type="POOLED">
                <property name="driver" value="${driver}"/>
                <property name="url" value="${url}"/>
                <property name="username" value="${username}"/>
                <property name="password" value="${password}"/>
            </dataSource>
        </environment>
    </environments>
    <!-- have some troubles with maven so I have to put the xml under resource-folder
    otherwise the file can not be found -->
    <mappers>
    <!-- 我的配置文件都是放在 resource文件夹下的  mybatisXML 文件夹里面的 -->
        <mapper resource="mybatisXML/HiveETLLogs.xml"/>
        <mapper resource="mybatisXML/LogMapper.xml"/>
        <!-- can not find when ~ running <mapper resource="com/lms/etl/spark/pojo/HiveETLLogs.xml"/>-->

    </mappers>
</configuration>
```



## 原始DAO层开发方式

上以下配置文件，和Java POJO

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">

<mapper namespace="com.lms.etl.spark.pojo">
    <!-- id 是 我们调用的时候要指定执行某个 sql语句的参数， 然后 resultType 可以是 int，String，java对象等类型，java对象的话，配置里面要能找到 -->
    <select id="listLogs" resultType="HiveETLLogs">
        select * from   hiveetllogs
    </select>
    <insert id="addLogs" parameterType="HiveETLLogs">
        insert into hiveetllogs (orig_hdfs_path,hive_db,hive_table,is_history,is_partition,is_duplicate_removal,start_time,end_time,input_num,accepted_num,is_error,log_describe)
      values(#{origHdfsPath},#{hiveDb},#{hiveTable},#{isHistory},#{isPartition},#{isDuplicateRemoval},#{startTime},#{endTime},#{inputNum},#{acceptedNum},#{isError},#{logDescribe})
    </insert>
    <delete id="deleteLogs" parameterType="HiveETLLogs">
        delete from hiveetllogs where start_time=#{startTime}
    </delete>
    <update id ="updateLogs" parameterType="HiveETLLogs">
        udpate hiveetllogs set log_describe = 'unknown error' where start_time = #{startTime}
    </update>
    <!--getLog is a test : cooperation with interfact pojo.LogMapper-->
    <select id="getLog" parameterType="long" resultType="HiveETLLogs">
        select * from hiveetllogs where input_num=#{inputNum}
    <!-- <insert id="addLogs" parameterType="HiveETLLogs">
        insert into hiveetllogs (orig_hdfs_path,hive_db,hive_table,is_history,is_partition,is_duplicate_removal,start_time,end_time,input_num,accepted_num,is_error,log_describe)
      values(#{orig_hdfs_path},#{hive_db},#{hive_table},#{is_history},#{is_partition},#{is_duplicate_removal},#{start_time},#{end_time},#{input_num},#{accepted_num},#{is_error},#{log_describe})
    </insert>
    <delete id="deleteLogs" parameterType="HiveETLLogs">
        delete from hiveetllogs where start_time=#{start_time}
    </delete>
    <update id ="updateLogs" parameterType="HiveETLLogs">
        udpate hiveetllogs set log_describe = 'unknown error' where start_time = #{start_time}
    </update> -->
    <!--getLog is a test : cooperation with interfact pojo.LogMapper-->
    <!-- <select id="getLog" parameterType="long" resultType="HiveETLLogs">
        select * from hiveetllogs where input_num=#{input_num}
    </select> -->
</mapper>

```

```java
// 即使一个普通 pojo 就是字段有点多而以！！！！
package com.lms.etl.spark.pojo;

import org.apache.ibatis.type.Alias;

import java.sql.Date;

/**
 * @author: MissingLi
 * @date: 3/25/18 2:48 PM
 * @Description:
 * @Modified by:
 */
// mybatis can still get the class even if we do not use Alias ,if we load the whole package in the config file
    //
@Alias("hiveETLLogs")
public class HiveETLLogs implements ETLLogs{
    // 这里的命名不规范， mybatis 是可以把 _  与 驼峰 映射出来的！！！！！ 主要代码标准
    // 配置文件里面开启 驼峰 与 _ 映射，然后这里面 所有的配置都改驼峰的
    // private String orig_hdfs_path ;
    // private String hive_db ;
    // private String hive_table;
    // private Boolean is_history;
    // private Boolean is_partition;
    // private Boolean is_duplicate_removal ;
    // private Date start_time ;
    // private Date end_time ;
    // private Long input_num ;
    // private Long accepted_num ;
    // private Boolean is_error ;
    // private String log_describe ;
    private String origHdfsPath ;
    private String hiveDb ;
    private String hiveTable;
    private Boolean isHistory;
    private Boolean isPartition;
    private Boolean isDuplicateRemoval ;
    private Date startTime ;
    private Date endTime ;
    private Long inputNum ;
    private Long acceptedNum ;
    private Boolean isError ;
    private String logDescribe ;
    // 后面是 getter 和 setter 就不写了，太多了
}

```

```java
//测试代码
// 这里要注意 结果集合的时候 selectList 的使用 还有 selectOne 和其他 select 的重载
        String resource = "mybatis-config.xml";
        InputStream input = Resources.getResourceAsStream(resource);
        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(input);
        SqlSession session = sqlSessionFactory.openSession();
        List<HiveETLLogs> cs = session.selectList("listLogs");
       for (HiveETLLogs c: cs
            ) {
           System.out.println(c.getInput_num());

       }
        HiveETLLogs logs = new HiveETLLogs();
        logs.setOrig_hdfs_path("123456");
        logs.setAccepted_num(8887L);
        session.insert("addLogs",logs);
        //执行insert之后需要 commit 才行
        session.commit();
        // 最后还有个  session.close();
```



## Mapper的用法

```xml
<!-- 这是mapper的 -->
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper
        PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">

<mapper namespace="com.lms.etl.spark.mapper.LogMapper">
    <select id="getLog" parameterType="long" resultType="HiveETLLogs">
        select * from hiveetllogs where input_num=#{inputNum}
    </select>
</mapper>
```

```java
// 接口
package com.lms.etl.spark.mapper;

import com.lms.etl.spark.pojo.HiveETLLogs;

import java.util.List;

/**
 * @author: MissingLi
 * @date: 26/03/18 15:05
 * @Description: todo : this is a test for MyBatis
 * @Modified by:
 */
public interface LogMapper {
    public List<HiveETLLogs> getLog(Long input_num);
}
// 返回值的类型 和 sql语句的类型要对应
// 抽象方法的名称，和sql语句的id要对应
// 这就是 mapper代理的方法！！！！！
// 底层方法应该 很强！！！！
```

```java
//前面的获取 session 和上边的例子一样
        /**
         * test for interface LogMapper
         **/
        LogMapper logMapper = session.getMapper(LogMapper.class);
        List<HiveETLLogs> log =logMapper.getLog(5L);
        for(HiveETLLogs lg :log){
            System.out.println(lg.getInputNum());
        }

        session.close();
```