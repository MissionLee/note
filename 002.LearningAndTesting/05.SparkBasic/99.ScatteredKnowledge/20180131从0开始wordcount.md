# 重新一次 wordcount

```note
包括，工程配置，程序，spark-submit执行等问题
```

# 新建项目

- 1. 存在问题的 scala+sbt  
    因为没弄明白sbt的配置，不知道怎么切换sbt的仓库位置，所以一直没弄明白怎么使用，所以切换到了maven上

- 2. 建立一个 scala + maven的 项目 这里要选择jdk 和 scala 的版本
  - 新建 scala - idea 项目
  - 添加框架支持 ： maven
  - maven 配置如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>groupId</groupId>
    <artifactId>SparkLearn</artifactId>
    <version>1.0-SNAPSHOT</version>
    <!-- 
    要注意这里定义了 两个变量
    我在最初选择项目的时候选择 scala的版本是 2.14
    与这里冲突了，导致了一些问题
    实际上 spark 的版本 和 我机器里面安装的版本也不同
    这里应当注意，不过貌似 spark版本不同并没有影响
    代码
     -->
    <properties>
        <spark.version>2.1.0</spark.version>
        <scala.version>2.11</scala.version>
    </properties>


    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

    </dependencies>

    <build>
        <plugins>

            <plugin>
                <groupId>org.scala-tools</groupId>
                <artifactId>maven-scala-plugin</artifactId>
                <version>2.15.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.0</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.19</version>
                <configuration>
                    <skip>true</skip>
                </configuration>
            </plugin>

        </plugins>
    </build>
</project>
```

- 在src目录下面，新建 WordCount文件夹
  - 右键 这个文件夹 Make Directionary as ->  Source root
- workcount 文件夹
  - 右键 这个文件夹  Create scala class
    - kind Object
    - 代码如下

```scala
import org.apache.spark.{SparkConf, SparkContext}

/**
  * First test wordcount program
  **/
object wordcount {
  def main(args: Array[String]){
      // 这个 word文件 事先自己创建好了
    //val inputFile = "/home/missingli/word";
    val inputFile="file:///home/missingli/word";
    val conf = new SparkConf().setAppName("wordcount").setMaster("local");
    val sc = new SparkContext(conf);
    val testFile = sc.textFile(inputFile);
    val wordCount = testFile.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey((a,b)=>a +b);
    wordCount.foreach(println)
  }
}
```

- 执行这个程序 run()
  - 这里出现了一个 class not found错误，指向 `val sc = new SparkContext(conf);`
  - 错误实际上和代码无关，因为创建工程时候的scala 版本不对，在 Project Structure 里面改变之后就可以正常运行并且得到结果

## 打包交给 spark-submit

- 打包
  - 打包 Project Structure -> Artifacts -> + -> Jar -> From Module with...
  - 弹出框中选择 main class -> 找到并选中 wordcount
  - OK 然后 output layout 中 把除了本体之外所有的依赖 删除了
  - Apply - > OK 
  
  - 主界面导航兰 Build -> artifacts -> build
  - 然后可以在项目目录 找到 out/artifacts/SparkLearn_jar 目录下找到这个 jar文件
- 执行
  - [root@localhost SparkLearn_jar]# spark-submit --class wordcount ./SparkLearn.jar
  - 这时候回报错 connection failed
  - 原因是 程序里面的文件路径 如果没有前缀 默认 hdfs上的，所以如下修改路径
    - `val inputFile = "file:///home/missingli/word";`
- 关于路径问题
  - 默认是从hdfs读取文件，也可以指定sc.textFile("路径").在路径前面加上hdfs://表示从hdfs文件系统上读
  - 本地文件读取 sc.textFile("路径").在路径前面加上file:// 表示从本地文件系统读，如file:///home/user/spark/README.md


## 程序运行结果都正常