
- 源码解析
  - 1. 参数 zeroValue 类型 U
  - 2. 参数函数1 seqOp (U,T)=>U
    - 用来对一个分区中的元素进行晕眩
  - 3. 参数函数2 comOp (U,U)=>U
    - 用来对分区之间的结果进行计算
  - 

```scala
 /**
   * Aggregate the elements of each partition, and then the results for all the partitions, using
   * given combine functions and a neutral "zero value". This function can return a different result
   * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U
   * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are
   * allowed to modify and return their first argument instead of creating a new U to avoid memory
   * allocation.
   *
   * @param zeroValue the initial value for the accumulated result of each partition for the
   *                  `seqOp` operator, and also the initial value for the combine results from
   *                  different partitions for the `combOp` operator - this will typically be the
   *                  neutral element (e.g. `Nil` for list concatenation or `0` for summation)
   * @param seqOp an operator used to accumulate results within a partition
   * @param combOp an associative operator used to combine results from different partitions
   */
  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope {
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance())
    val cleanSeqOp = sc.clean(seqOp)
    val cleanCombOp = sc.clean(combOp)
    val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)
    sc.runJob(this, aggregatePartition, mergeResult)
    jobResult
  }

  /**
   * Aggregates the elements of this RDD in a multi-level tree pattern.
   *
   * @param depth suggested depth of the tree (default: 2)
   * @see [[org.apache.spark.rdd.RDD#aggregate]]
   */
  def treeAggregate[U: ClassTag](zeroValue: U)(
      seqOp: (U, T) => U,
      combOp: (U, U) => U,
      depth: Int = 2): U = withScope {
    require(depth >= 1, s"Depth must be greater than or equal to 1 but got $depth.")
    if (partitions.length == 0) {
      Utils.clone(zeroValue, context.env.closureSerializer.newInstance())
    } else {
      val cleanSeqOp = context.clean(seqOp)
      val cleanCombOp = context.clean(combOp)
      val aggregatePartition =
        (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
      var partiallyAggregated = mapPartitions(it => Iterator(aggregatePartition(it)))
      var numPartitions = partiallyAggregated.partitions.length
      val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2)
      // If creating an extra level doesn't help reduce
      // the wall-clock time, we stop tree aggregation.

      // Don't trigger TreeAggregation when it doesn't save wall-clock time
      while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) {
        numPartitions /= scale
        val curNumPartitions = numPartitions
        partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex {
          (i, iter) => iter.map((i % curNumPartitions, _))
        }.reduceByKey(new HashPartitioner(curNumPartitions), cleanCombOp).values
      }
      partiallyAggregated.reduce(cleanCombOp)
    }
  }
```

- aggregate 和 reduce/fold有些类似
  - 复习一下
    - reduce 对数据进行 reduce
    - fold 对数据进行有初始值的 reduce（注意数据分区情况下存在的问题）
  - aggregate

首先还是用一个普通的示例：
```scala
    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9))
    val result = rdd.aggregate(0)((x,y)=>x+y,(x,y)=>x+y);
    println(result) //45
```

从参数函数 seqOp 可以看到， aggregate可以生成与RDD中元素不同类型的结果（RDD： T ，结果 ：U）

我们下面做一个简单测试

```scala
    val result2 = rdd.aggregate((0, 0))(
      (x, y) => (x._1 + 1, x._2 + y)
    ,
      (a, b) => (a._1 + b._1, a._2 + b._2))
    println(result2) //(9,45)
```

这里 我们的 类型 U 实际是一个 二元组，相当于 key-value的形式。

在处理过程中，要保证 zeroValue和来个函数的返回值保持同一个形式

稍微改动一下函数，看一下运行过程

```scala
    val result2 = rdd.aggregate((0, 0))(
      (x, y) => {
        println("x:"+x)
        (x._1 + 1, x._2 + y)}
    ,
      (a, b) => (a._1 + b._1, a._2 + b._2))
    println(result2) //(9,45)
    }
```

打印出来的流程

```note
x:(0,0)
x:(1,1)
x:(2,3)
x:(3,6)
x:(4,10)
x:(5,15)
x:(6,21)
x:(7,28)
x:(8,36)
```