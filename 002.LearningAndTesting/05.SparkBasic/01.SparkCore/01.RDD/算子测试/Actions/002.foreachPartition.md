# 1

```scala
// 源码
 /**
   * Applies a function f to each partition of this RDD.
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))
  }
```

输入
  - 一个函数
    - 入参是一个 Iterator
    - 无返回值
作用
  - 内部会把一个分区的数据的Iterator当作参数传给开发者定义的函数，完成相映的工作

```测试代码
object TestForEachPartition{
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val a1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip1.txt")
  val a2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip2.txt")
  val b1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip3.txt")
  val b2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip4.txt")

  def main(args: Array[String]): Unit = {
    val rdd = a1++a2++b1++b2
    rdd.foreachPartition(
      r=>{
        var x = ""
        while (r.hasNext){
          x=x+r.next().toString
        }
        println(x)
      }
    )

  }
}
```

测试结果
```scala
A1A2A3A4
A5A6
B1B2B3B4
B5B6
```

- 可以看到，每个分区被连接成了字符串