```scala
  /**
   * Reduces the elements of this RDD using the specified commutative and
   * associative binary operator.
   */
  def reduce(f: (T, T) => T): T = withScope {
    val cleanF = sc.clean(f)
    val reducePartition: Iterator[T] => Option[T] = iter => {
      if (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } else {
        None
      }
    }
    var jobResult: Option[T] = None
    val mergeResult = (index: Int, taskResult: Option[T]) => {
      if (taskResult.isDefined) {
        jobResult = jobResult match {
          case Some(value) => Some(f(value, taskResult.get))
          case None => taskResult
        }
      }
    }
    sc.runJob(this, reducePartition, mergeResult)
    // Get the final result out of our Option, or throw an exception if the RDD was empty
    jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))
  }
```

- 作用： 给定一个函数，输入 rdd两个元素，返回一个同类型元素

-测试代码
```scala
package basic

import org.apache.spark.sql.SparkSession

/**
  * 
  */
object TestReduce {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val a1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip1.txt")
  val a2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip2.txt")
  val b1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip3.txt")
  val b2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip4.txt")

  def main(args: Array[String]): Unit = {
    println("---------String-----")
    val rdd1 = a1 ;
    val rdd2 =rdd1.reduce(_+_)
    println(rdd2)
    println("----------Int-----------")
    val c = sc.parallelize(1 to 10)
    val c2 = c.reduce(_+_)
    println(c2)
    println("----------Int with partition--------")
    val c3 = c++c++c;
    val c4 =c3.reduce(_+_)
    println(c4)

  }
}
```

代码输出

```note
---------String-----
A1A2A3A4
----------Int-----------
55
----------Int with partition--------
165
```

- 结论
  - 按照指定的函数进行计算
  - 会计算整个rdd（跨分区）
  - 因为需要用计算结果带入提供的函数连续计算，所以限制了返回类型必须和输入类型一样
- 作用
  - 可以用来数字求和
  - 可以用来求最值
  - 其他

```scala
//求最值写法
    println("----------find the largest number-----------")
    val c5 = c.reduce((x,y)=>if(x>y) x else y)
    println(c5)
```


```note
----------find the largest number-----------
10
```