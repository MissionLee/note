 
 ```scala
 /**
   * Return an iterator that contains all of the elements in this RDD.
   *
   * The iterator will consume as much memory as the largest partition in this RDD.
   *
   * @note This results in multiple Spark jobs, and if the input RDD is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input RDD should be cached first.
   */
  def toLocalIterator: Iterator[T] = withScope {
    def collectPartition(p: Int): Array[T] = {
      sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head
    }
    (0 until partitions.length).iterator.flatMap(i => collectPartition(i))
  }
```

返回一个包含RDD所有元素的iterator。iterator占用RDD中最大分区的内存大小

源码分析
```scala
//首先是一个内部方法
    def collectPartition(p: Int): Array[T] = {
      sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head
    }
    
```
从方法名可以看出，这是用来获取分区的
其中 sc.runJob
```scala
// runJob有许多重载，我们看直接使用的这一个
  /**
   * Run a function on a given set of partitions in an RDD and return the results as an array.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: Iterator[T] => U,
      partitions: Seq[Int]): Array[U] = {
    val cleanedFunc = clean(func)
    runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions)
  }
  //在RDD的指定的某些分区中执行给定的函数，返回一个结果数组
  // runJob的体系和DAG相关，不再往后查找了（点了点代码，太深了）
```
再toLocalIterator代码中，传递给runJob的方法是
```scala
(iter: Iterator[T]) => iter.toArray
// 这个方法是交给 spark 来执行的，spark会把自己的rdd的iterator带入这个函数，返回的是这个iterator的 .toArray => 完成的工作就是整个RDD元素转未了Array
```

回到最外层的代码中
```scala
  def toLocalIterator: Iterator[T] = withScope {
    def collectPartition(p: Int): Array[T] = {
      sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head
    }
    (0 until partitions.length).iterator.flatMap(i => collectPartition(i))
  }
```
其中
```scala
    (0 until partitions.length).iterator.flatMap(i => collectPartition(i))
// 0 until 10 获取的是一个 Range 
// 下面简单看一下Range的定义 不做过多解释
class Range(val start: Int, val end: Int, val step: Int)
extends scala.collection.AbstractSeq[Int]
   with IndexedSeq[Int]
   with scala.collection.CustomParallelizable[Int, ParRange]
   with Serializable
```
这里有一点就是 iterator 中的flatMap
```scala
  /** Creates a new iterator by applying a function to all values produced by this iterator
   *  and concatenating the results.
   *
   *  @param f the function to apply on each element.
   *  @return  the iterator resulting from applying the given iterator-valued function
   *           `f` to each value produced by this iterator and concatenating the results.
   *  @note    Reuse: $consumesAndProducesIterator
   */
  def flatMap[B](f: A => GenTraversableOnce[B]): Iterator[B] = new AbstractIterator[B] {
//missingli： 这实际上就是一个 嵌套的 iterator，外层iterator存储的元素是一个iterator，每次外部访问，向循环 外层iterator存储的iterator，然后 循环外层iterator

    // 要求输入参数 返回一个 可遍历的 类型
    // 返回一个iterator
    
    // 这个Iterator是一个 AbstractIterator的 实例

    private var cur: Iterator[B] = empty
    //获得一个空的 iterator

    private def nextCur() { cur = f(self.next()).toIterator }
    // nextCur  

    def hasNext: Boolean = {
      // Equivalent to cur.hasNext || self.hasNext && { nextCur(); hasNext }
      // but slightly shorter bytecode (better JVM inlining!)
      while (!cur.hasNext) {  //如果当前存储的iterator 没有元素了 就把外部的元素向下走一位置，并且把 
        if (!self.hasNext) return false//遍历外层 如果外层没有了，就真的没有了
        nextCur()//当前到头了，外部还每到头，就  =>外部的下个元素的 iterator 指向 cur
      }
      true
    }
    def next(): B = (if (hasNext) cur else empty).next()
  }
  // 对一个 iterator 的每个元素执行一个function，并把所有的结果汇总返回成一个 iterator

  // 内外代码整合大概是下面这个逻辑
    // 1.collectPartition 把RDD每个分区转成一个Array
    // 2.通过Range的flatMap,获得一个(分区Iterator-分区Array.toIterator) 复合的Iterator
  
```


测试代码

```scala
object TestToLocalIterator {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val a1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip1.txt")
  val a2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip2.txt")
  val b1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip3.txt")
  val b2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip4.txt")

  def main(args: Array[String]): Unit = {
    val rdd = a1++a2++b1++b2
    val iter = rdd.toLocalIterator
    while (iter.hasNext){
      println(iter.next())
    }
  }
}
```
测试结果
```note
A1
A2
A3
A4
A5
A6
B1
B2
B3
B4
B5
B6
```