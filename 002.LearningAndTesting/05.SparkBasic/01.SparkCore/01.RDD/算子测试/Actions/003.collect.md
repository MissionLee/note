# 

```scala
  /**
   * Return an array that contains all of the elements in this RDD.
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   */
  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }

    /**
   * Return an RDD that contains all matching values by applying `f`.
   */
  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    filter(cleanF.isDefinedAt).map(cleanF)
  }
```

返回一个包含RDD中所有的元素的数组

注意： 因为所有数据会在这个步骤装载到内存中，我们希望这个数组不要太大

>基础方法测试

```scala
object TestCollect {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val a1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip1.txt")
  val a2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip2.txt")
  val b1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip3.txt")
  val b2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip4.txt")

  def main(args: Array[String]): Unit = {
    val rdd = a1++a2++b1++b2
    val array = rdd.collect()
    for(str<-array){
      println(str)
    }

  }
}
```

输出

```note
A1
A2
A3
A4
A5
A6
B1
B2
B3
B4
B5
B6
```

>带过滤的方法测试

在源代码中可以看到，collect实际上把我们提供的函数传-一个标准偏函数-递给了 filter

filter实际上是要求对元素进行判断并返回布尔值

```scala
// 自己实现的一个偏函数，因为spark 总使用，需要可序列化
class MyParitialFunction extends Serializable   with PartialFunction [Any,String]{
  override def isDefinedAt(x: Any): Boolean = if(x.toString.startsWith("A")) true else false

  override def apply(v1: Any): String = v1.toString
}
// 测试如下
object TestCollect {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val a1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip1.txt")
  val a2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip2.txt")
  val b1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip3.txt")
  val b2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip4.txt")

  def main(args: Array[String]): Unit = {
    val rdd4 = sc.parallelize(List(1, 2, 3))
    val rdd = a1 ++ a2 ++ b1 ++ b2
    val array = rdd.collect()
    for (str <- array) {
      println(str)
    }

    val array2 = rdd.collect { case a: String => {
      if (a.startsWith("A")) a
    }
    }
    val array3 = rdd.collect(new MyParitialFunction)


    println(" array2")
    for (str <- array2) {
      println(str)
    }
    println("array 3")
    for (str <- array3) {
      println(str)
    }
  }
}
```

结果

```note
A1
A2
A3
A4
A5
A6
B1
B2
B3
B4
B5
B6
 array2
A1
A2
A3
A4
A5
A6
()
()
()
()
()
()
array 3
A1
A2
A3
A4
A5
A6
```

- 结论
  - 在List类型的collect中给定一个 case实现的偏函数是一种很方便的提取指定类型数据，然后操作返回的方法
  - 在SparkRDD中这样操作我个人目前没发发现太大的一以，因为RDD中一般存储相同类型的内容，没有必要用偏函数的过滤特性，而如果我们需要筛选，在之前多一部 filter函数，可能是更优秀的选择