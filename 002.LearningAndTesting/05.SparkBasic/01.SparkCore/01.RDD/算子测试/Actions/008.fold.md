- 通过给定的 associative function(结合函数)和一个"zero value"。对每个分区的元素进行聚合得到相应结果，然后对每个分区的结果进行聚合 
- fold与reduce类似，区别是可以给定一个零值/初始值
```scala
  /**
   * Aggregate the elements of each partition, and then the results for all the partitions, using a
   * given associative function and a neutral "zero value". The function
   * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object
   * allocation; however, it should not modify t2.
   *
   * This behaves somewhat differently from fold operations implemented for non-distributed
   * collections in functional languages like Scala. This fold operation may be applied to
   * partitions individually, and then fold those results into the final result, rather than
   * apply the fold to each element sequentially in some defined ordering. For functions
   * that are not commutative, the result may differ from that of a fold applied to a
   * non-distributed collection.
   *
   * @param zeroValue the initial value for the accumulated result of each partition for the `op`
   *                  operator, and also the initial value for the combine results from different
   *                  partitions for the `op` operator - this will typically be the neutral
   *                  element (e.g. `Nil` for list concatenation or `0` for summation)
   * @param op an operator used to both accumulate results within a partition and combine results
   *                  from different partitions
   */
  def fold(zeroValue: T)(op: (T, T) => T): T = withScope {
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())
    val cleanOp = sc.clean(op)
    val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp)
    val mergeResult = (index: Int, taskResult: T) => jobResult = op(jobResult, taskResult)
    sc.runJob(this, foldPartition, mergeResult)
    jobResult
  }
```

测试代码

```scala
object TestFold {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val x = List(1,2,3,4,5,6,7,8,9);
  val rdd1 = sc.parallelize(x,1)
  val rdd2 = sc.parallelize(x,2)
  val a1 =rdd1.fold(100)((x,y)=>x+y)
  val a2 =rdd2.fold(100)((x,y)=>x+y)


  def main(args: Array[String]): Unit = {
    println("a1:" + a1)
    print("a2:" + a2)
  }
}
```
测试结果

```note
a1:245
a2:345
```

- 结论
  - a1 所对应的rdd 只有一个分区
    - 首先 计算 100（零数值）+1，2，3，4，5，6，7，8，9 得到 145
    - 然后计算 100 + 各个分区的结果（因为只有一个分区，所以是 145）
    - 最终结果 245
  - a2 对应rdd 有来嗯个分区
    - 首先每个分区计算  100 + 分区内容合
    - 然后计算分区总赫 100 + 分区1结果 + 分区2结果
    - 就是  100 + 100+ 分区1内容 + 10 + 分区2 内容  （其中 分区1内容+ 分区2 内容为 45）
    - 结果 345