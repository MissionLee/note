
SparkSQL
  - core： org.apache.spark.sql
    - api python语言，r语言的接口
    - catalog - Catalog interface for Spark. To access this, use `SparkSession.catalog`. 用来获取当前spark的库/表/function（SparkSQL使用的时候会注册这些东西）
    - execution - 里面是具体实现内容，这个要细看，包括各种operator，各种数据格式的支持之类的
      - aggregate
      - arrow
      - columnar
      - command
      - datasources
      - 等等
    - expressions 
      - scalalong.typed.scala : dataset的类型安全的functions
      - Aggregator：udf-aggregations的基础类
      - ReduceAggregator ： extend Aggregator
      - udaf ： The base class for implementing user-defined aggregate functions (UDAF).
      - UserDefinedFunction：（case class）
        - 例子
        ```scala
           // Define a UDF that returns true or false based on some numeric score.
           val predict = udf((score: Double) => score > 0.5)
        
           // Projects a column that adds a prediction column based on     the score column.
           df.select( predict(df("score")) )
        ```
        - window : Utility functions for defining window in DataFrames.
          - 里面是 partitionBy/orderBy 这样的操作
        - WindowSpec：A window specification that defines the partitioning, ordering, and frame boundaries.
    - internal
      - 应该是一些辅助类，里面由一个 CatalogImpl
      - 还有 Hive SerDe (Hive Serialize/Deserilize)
    - jdbc
      - 这个包里面都是写 XX数据库方言，应该是用来识别 jdbc连接类型的
      - 类里面有一些简单方法，比如判断连接 url 是否以某个数据库要求的开头
      - 还不是很肯定这部分内容
    - sources
      - 提供了重要的 DataSourceRegister 这个trait
      - 还有 RelationProvider
      - 等等 
      - StreamSourceProvider
    - streaming
      - streaming的核心内容
      - DataStreamReader
      - DataStreamWriter
      - 还有StreamingQuery相关的多个类
    - util
    - 直接核心类
      - [Column] - Dataframe运算抽象出一个结果的代指，可以用一个表达式来构造（假设由一列叫做 a）：  $"a" + 1
      - [DataFrameNaFunctions] - Functionality for working with missing data in `DataFrame`s.
      - [DataFrameReader] - Interface used to load a [[Dataset]] from external storage systems (e.g. file systems,key-value stores, etc). Use `SparkSession.read` to access this.
      - [DataFrameStatFunctions] - Statistic functions for `DataFrame`s.
        - ！！！！！！！！！！！！！！！！
      - [DataFrameWriter] - Interface used to write a [[Dataset]] to external storage systems，Use `Dataset.write` to access this.
      - [Dataset]
      - [DatasetHolder] -A container for a [[Dataset]], used for implicit conversions in Scala. 需要import spark.implicits._
      - [ExperimentalMethods]
      - [ForeachWriter] - A class to consume data generated by a `StreamingQuery`. Typically this is used to send the generated data to external systems.
        - !!!!!!!!!!!!!!!!!!
      - [functions] - Functions available for DataFrame operations. 实际上里面基本都是定义在 Column类里面的方法的代理
      - [keyalueGroupedDataset] - A [[Dataset]] has been logically grouped by a user specified grouping key.
      - [RelationalGroupedDataset]  - A set of methods for aggregations on a `DataFrame`, created by [[Dataset#groupBy groupBy]], [[Dataset#cube cube]] or [[Dataset#rollup rollup]] (and also `pivot`).
        - ！！！！！！！！！！！！！！
      - [RuntimeConfig] - Runtime configuration interface for Spark. To access this, use `SparkSession.conf`.
      - [SparkSession] - The entry point to programming Spark with the Dataset and DataFrame API.
      - [SparkSessionExtensions] - Holder for injection points to the [[SparkSession]].
      - [sql] 
        - 简单粗暴
        ```scala
        package object sql {

          /**
           * Converts a logical plan into zero or more SparkPlans.  This API is exposed for experimenting
           * with the query planner and is not designed to be stable across spark releases.  Developers
           * writing libraries should instead consider using the stable APIs provided in
           * [[org.apache.spark.sql.sources]]
           */
          @DeveloperApi
          @InterfaceStability.Unstable
          type Strategy = SparkStrategy

          type DataFrame = Dataset[Row]
        }
        ```
      - [SQLContext] - The entry point for working with structured data (rows and columns) in Spark 1.x.
      - [SQLImplicits] - A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
        - 把普通的scala对象，转为dataset体系里面对应的对象,主要是把一些类和 Encoder结合起来，也有其他一些语法糖
          - 例子： 
          ```scala
          implicit class StringToColumn(val sc: StringContext) {
            def $(args: Any*): ColumnName = {
              new ColumnName(sc.s(args: _*))
            }
          }
          ```

      - [UDFRegistration] - Functions for registering user-defined functions. Use `SparkSession.udf` to access this:
        - ！！！！！！！！！！！！！！！！！！
  - hive
    - Spark在这里实现了hive的一整套东西
  - hive-thrift-server
    - 这个应该是 SparkSQL 用 hive-thrift-server实现的一套
  - catalyst
    - 核心引擎！
