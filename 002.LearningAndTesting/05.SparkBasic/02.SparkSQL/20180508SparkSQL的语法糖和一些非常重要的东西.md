# 我们寻根溯源

- 我们再 用sparkSession的时候，总是要来这么一句
  - import spark.implicits._
  - 这里就来看看 implicits._ 是个什么
    - SparkSession 中由这样一个内部object
    ```scala
      /**
       * :: Experimental ::
       * (Scala-specific) Implicit methods available    in Scala for converting
       * common Scala objects into `DataFrame`s.
       *
       * {{{
       *   val sparkSession =     SparkSession.builder.getOrCreate()
       *   import sparkSession.implicits._
       * }}}
       *
       * @since 2.0.0
       */
      @Experimental
      @InterfaceStability.Evolving
      object implicits extends SQLImplicits with    Serializable {
        protected override def _sqlContext: SQLContext    = SparkSession.this.sqlContext
      }
    ```
    - 这里的解释是 引入隐式转换，把普通object转为dataframe
  - 在此之外，要注意 extends SQLImplicits这一句
  - 下面就来看看这个类里面都是什么（以下内容，都是这个类里面的源码）
  - 让人困惑的语法塘 $ 找到了
  ```scala
  //Converts $"col name" into a [[Column]].
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }
  ```
  - [Scala implicit class](../../06.ScalaBasic/20180508-implicit-class.md)
  - 这里定义了一个 $ 方法，把$col转换成 列
  - 这就是 df.select($"key1") 这一句的原理