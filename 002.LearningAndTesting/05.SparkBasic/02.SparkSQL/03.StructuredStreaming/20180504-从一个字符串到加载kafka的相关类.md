
- 这篇文章的题目可能有些费解，我来解释一下：
  - Spark再创建 structured streaming 的时候  lines = spark.readStream.format("kafka") 只是设置了一个字符串，就能够获取相关的操作类。`起初我以未程序内部由判断，用kafka这个名字，找到对应的类全名，然后加载进来。再我的设想汇总，应该维护一个kafka-org.apache.spark.sql.kafka.....这样的映射，但是实际上没有`
  - 最后发现了相关方法，用的是 Java的 serviceloader源里，来自JDK1.6版本的特性，关于service loader 再java部分的文章里面应该由收录。


```scala
 def load(): DataFrame = {
    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) {
      throw new AnalysisException("Hive data source can only be used with tables, you can not " +
        "read files of Hive data source directly.")
    }
    // 我们在 .format的时候他提供 名称 为kafka  下面 ds 会创建kafka的实现类的实例
    val ds = DataSource.lookupDataSource(source, sparkSession.sqlContext.conf).newInstance()
    val options = new DataSourceOptions(extraOptions.asJava)
    // We need to generate the V1 data source so we can pass it to the V2 relation as a shim.
    // We can't be sure at this point whether we'll actually want to use V2, since we don't know the
    // writer or whether the query is continuous.
    val v1DataSource = DataSource(
      sparkSession,
      userSpecifiedSchema = userSpecifiedSchema,
      className = source,
      options = extraOptions.toMap)
    val v1Relation = ds match {
      case _: StreamSourceProvider => Some(StreamingRelation(v1DataSource))
      case _ => None
    }
    // 根据 数据源的不同 
    ds match {
      case s: MicroBatchReadSupport =>
        val tempReader = s.createMicroBatchReader(
          Optional.ofNullable(userSpecifiedSchema.orNull),
          Utils.createTempDir(namePrefix = s"temporaryReader").getCanonicalPath,
          options)
        Dataset.ofRows(
          sparkSession,
          StreamingRelationV2(
            s, source, extraOptions.toMap,
            tempReader.readSchema().toAttributes, v1Relation)(sparkSession))

            // 下面贴了一点 kafkaSourceProvider 的定义部分的代码，
            // 总之 kafka 会满足 ComtinuousReadSupport
      case s: ContinuousReadSupport =>
        val tempReader = s.createContinuousReader(
          Optional.ofNullable(userSpecifiedSchema.orNull),
          Utils.createTempDir(namePrefix = s"temporaryReader").getCanonicalPath,
          options)
        Dataset.ofRows(
          sparkSession,
          StreamingRelationV2(
            s, source, extraOptions.toMap,
            tempReader.readSchema().toAttributes, v1Relation)(sparkSession))
      case _ =>
        // Code path for data source v1.
        Dataset.ofRows(sparkSession, StreamingRelation(v1DataSource))
    }
  }

```
KafkaSourceProvider的定义
```scala
private[kafka010] class KafkaSourceProvider extends DataSourceRegister
    with StreamSourceProvider
    with StreamSinkProvider
    with RelationProvider
    with CreatableRelationProvider
    with StreamWriteSupport
    with ContinuousReadSupport
    with Logging 
```


下面是 加载 一个指定数据源的过程


```scala
  /** Given a provider name, look up the data source class definition. */

  // 假设我们在创建的时候给定的名称为 kafka，最后这里的 provider 就是 kafka
  def lookupDataSource(provider: String, conf: SQLConf): Class[_] = {
      // 有一些类型可能传进来不同的provider名称，但是实际上是同一个东西
      // 这里 backwardCompatibitityMap 作用就是识别这些内容
      // 不过里面都是些 json jdbc orc 什么的，我们真正要看的是kafka
      // 这里 provider1 比较了一圈之后  provider1 = kafka
    val provider1 = backwardCompatibilityMap.getOrElse(provider, provider) match {
      case name if name.equalsIgnoreCase("orc") &&
          conf.getConf(SQLConf.ORC_IMPLEMENTATION) == "native" =>
        classOf[OrcFileFormat].getCanonicalName
      case name if name.equalsIgnoreCase("orc") &&
          conf.getConf(SQLConf.ORC_IMPLEMENTATION) == "hive" =>
        "org.apache.spark.sql.hive.orc.OrcFileFormat"
      case name => name
    }
    // provider2 = kafka.DefaultSource
    val provider2 = s"$provider1.DefaultSource"
    val loader = Utils.getContextOrSparkClassLoader
    // 这里 serviceLoader 是一个完整的知识点
    val serviceLoader = ServiceLoader.load(classOf[DataSourceRegister], loader)
    // 这里利用serviceLoader技术找到 kafka
    // 这也是为什么 我最初在  没又添加依赖，就尝试指定kafka 的时候会在运行的时候报错
```
[ServiceLoader](../../12.JavaBasic/20180504-ServiceLoader.md)
```scala
    try {
        // 在调式运行的过程中，可以看到 serviceLoader中的 providers(一个HashMap在运行过程中不断变大，idea 的 step into条时功能-F7)
        // 最终会找到 kafka的相关的类
        
      serviceLoader.asScala.filter(_.shortName().equalsIgnoreCase(provider1)).toList 
      
      // 下面 会对 这个 kafka的类进行一些判断 然后 得到最终返回
      match {
        // the provider format did not match any given registered aliases
        case Nil =>
          try {
            Try(loader.loadClass(provider1)).orElse(Try(loader.loadClass(provider2))) match {
              case Success(dataSource) =>
                // Found the data source using fully qualified path
                dataSource
              case Failure(error) =>
                if (provider1.startsWith("org.apache.spark.sql.hive.orc")) {
                  throw new AnalysisException(
                    "Hive built-in ORC data source must be used with Hive support enabled. " +
                    "Please use the native ORC data source by setting 'spark.sql.orc.impl' to " +
                    "'native'")
                } else if (provider1.toLowerCase(Locale.ROOT) == "avro" ||
                  provider1 == "com.databricks.spark.avro") {
                  throw new AnalysisException(
                    s"Failed to find data source: ${provider1.toLowerCase(Locale.ROOT)}. " +
                    "Please find an Avro package at " +
                    "http://spark.apache.org/third-party-projects.html")
                } else {
                  throw new ClassNotFoundException(
                    s"Failed to find data source: $provider1. Please find packages at " +
                      "http://spark.apache.org/third-party-projects.html",
                    error)
                }
            }
          } catch {
            case e: NoClassDefFoundError => // This one won't be caught by Scala NonFatal
              // NoClassDefFoundError's class name uses "/" rather than "." for packages
              val className = e.getMessage.replaceAll("/", ".")
              if (spark2RemovedClasses.contains(className)) {
                throw new ClassNotFoundException(s"$className was removed in Spark 2.0. " +
                  "Please check if your library is compatible with Spark 2.0", e)
              } else {
                throw e
              }
          }
        case head :: Nil =>
          // there is exactly one registered alias
          head.getClass
        case sources =>
          // There are multiple registered aliases for the input. If there is single datasource
          // that has "org.apache.spark" package in the prefix, we use it considering it is an
          // internal datasource within Spark.
          val sourceNames = sources.map(_.getClass.getName)
          val internalSources = sources.filter(_.getClass.getName.startsWith("org.apache.spark"))
          if (internalSources.size == 1) {
            logWarning(s"Multiple sources found for $provider1 (${sourceNames.mkString(", ")}), " +
              s"defaulting to the internal datasource (${internalSources.head.getClass.getName}).")
            internalSources.head.getClass
          } else {
            throw new AnalysisException(s"Multiple sources found for $provider1 " +
              s"(${sourceNames.mkString(", ")}), please specify the fully qualified class name.")
          }
      }
    } catch {
      case e: ServiceConfigurationError if e.getCause.isInstanceOf[NoClassDefFoundError] =>
        // NoClassDefFoundError's class name uses "/" rather than "." for packages
        val className = e.getCause.getMessage.replaceAll("/", ".")
        if (spark2RemovedClasses.contains(className)) {
          throw new ClassNotFoundException(s"Detected an incompatible DataSourceRegister. " +
            "Please remove the incompatible library from classpath or upgrade it. " +
            s"Error: ${e.getMessage}", e)
        } else {
          throw e
        }
    }
  }
```