## Archives

`合并小文件`

hadoop Archives可以使用archive工具创建，同上一篇讲的distcp一样，archive也是一个mapreduce任务。首先我们先来看下我的目录结构：

```bash
[hadoop@namenode ~]$hadoop fs -lsr
drwxr-xr-x   - hadoop supergroup          0 2013-06-20 12:37 /user/hadoop/har
drwxr-xr-x   - hadoop supergroup          0 2013-05-23 11:35 /user/hadoop/input
-rw-r--r--   2 hadoop supergroup     888190 2013-05-23 11:35 /user/hadoop/input/1901
-rw-r--r--   2 hadoop supergroup     888978 2013-05-23 11:35 /user/hadoop/input/1902
-rw-r--r--   2 hadoop supergroup        293 2013-06-02 17:44 /user/hadoop/news.txt
```

我们通过archive工具才对该目录进行归档

`hadoop archive -archiveName input.har -p /user/hadoop/ input har`

archiveName指定archive的文件名，-p代表父目录，可以把多个目录文件放到archive里，我们来看下创建好的har文件。

```bash
[hadoop@namenode ~]$hadoop fs -ls har
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2013-06-20 12:38 /user/hadoop/har/input.har
[hadoop@namenode ~]$hadoop fs -ls har/input.har
Found 4 items 
-rw-r--r--   2 hadoop supergroup          0 2013-06-20 12:38 /user/hadoop/har/input.har/_SUCCESS
-rw-r--r--   5 hadoop supergroup        272 2013-06-20 12:38 /user/hadoop/har/input.har/_index
-rw-r--r--   5 hadoop supergroup         23 2013-06-20 12:38 /user/hadoop/har/input.har/_masterindex
-rw-r--r--   2 hadoop supergroup    1777168 2013-06-20 12:38 /user/hadoop/har/input.har/part-0
```

这里可以看到har文件包括，两个索引文件，多个part文件，这里只显示一个。part文件是多个原文件的集合，根据index文件去找到原文件。\
如果用har uri去访问的话，这些文件就会隐藏起来，只显示原文件

```bash
[hadoop@namenode ~]$hadoop fs -lsr har:///user/hadoop/har/input.har  
drwxr-xr-x   - hadoop supergroup          0 2013-05-23 11:35 /user/hadoop/har/input.har/input  
-rw-r--r--   2 hadoop supergroup     888978 2013-05-23 11:35 /user/hadoop/har/input.har/input/1902  
-rw-r--r--   2 hadoop supergroup     888190 2013-05-23 11:35 /user/hadoop/har/input.har/input/1901  
```

还可以象普通文件系统那样访问har下一级的文件

```bash
[hadoop@namenode ~]$hadoop fs -lsr har:///user/hadoop/har/input.har/input  
-rw-r--r--   2 hadoop supergroup     888978 2013-05-23 11:35 /user/hadoop/har/input.har/input/1902  
-rw-r--r--   2 hadoop supergroup     888190 2013-05-23 11:35 /user/hadoop/har/input.har/input/1901  
```

如果要远程访问的话可以使用以下命令

```bash
[hadoop@namenode ~]$hadoop fs -lsr har://hdfs-namenode:9000/user/hadoop/har/input.har/input  
-rw-r--r--   2 hadoop supergroup     888978 2013-05-23 11:35 /user/hadoop/har/input.har/input/1902  
-rw-r--r--   2 hadoop supergroup     888190 2013-05-23 11:35 /user/hadoop/har/input.har/input/1901 
```

har开头说明时har文件系统，hdfs-域名:端口，har文件系统进行转换直到har文件末位，例子中会转换为hdfs://namenode:9000/user/hadoop/har/input.har,剩余的部分仍然用archive方式打开：input
删除文件相对简单，但需要递归删除，否则报错

```bash
[hadoop@namenode ~]$hadoop fs -rmr har/input.har  
Deleted hdfs://192.168.115.5:9000/user/hadoop/har/input.har  
```

## 限制

archive文件有一些限制条件：

- 1.创建archive文件要消耗和原文件一样多的硬盘空间
- 2.archive文件不支持压缩，尽管archive文件看起来象已经被压缩过了。
- 3.archive文件一旦创建就无法改变，这就意味这你要改一些东西的话，你需要创新创建archive文件
- 4.虽然解决了namenode的内存空间问题，但是在执行mapreduce时，会把多个小文件交给同一个mapreduce去split，这样明显是低效的解决namenode内存的问题可以参照之前的文章中的hdfs federation。