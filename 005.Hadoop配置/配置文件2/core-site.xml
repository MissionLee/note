<?xml version="1.0" encoding="UTF-8"?>

<!--Autogenerated by Cloudera Manager-->
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://BigData-02:8020</value>
    <!--  -->
  </property>
  <property>
    <name>fs.trash.interval</name>
    <value>1</value>
    <!-- 文件废弃表示设定 0 为禁止此功能 -->
  </property>
  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</value>
    <!-- 压缩解压方式 -->
  </property>
  <property>
    <name>hadoop.security.authentication</name>
    <value>simple</value>
    <!-- 无认证或认证设置 -->
  </property>
  <property>
    <name>hadoop.security.authorization</name>
    <value>false</value>
    <!-- 服务端认证开启 -->
  </property>
  <property>
    <name>hadoop.rpc.protection</name>
    <value>authentication</value>
    <!-- remote procduure call 远程过程调用 -->
  </property>
  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>DEFAULT</value>
    <!-- 将kerberos规则,映射到本地用户名称(操作系统用户账号) -->
    <!-- http://www.linuxidc.com/Linux/2016-09/134948.htm hadoop kerberos 官方配置详解 -->
  </property>



  <!-- 下面就是代理用户 superuser  -->
  <property>
    <name>hadoop.proxyuser.oozie.hosts</name>
    <value>*</value>
    <!-- 代理用户 -->
    <!-- hadoop.proxyuser.*.hosts -->
    <!-- http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html -->
    <!--当前: 和下面的一个属性配合,指定 oozie用户,可以代理主机 *(或者某个host)上面 属于用户组*(下面那一条中的*)所有的用户  -->
  </property>
  <property>
    <name>hadoop.proxyuser.oozie.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.mapred.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.mapred.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.flume.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.flume.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.HTTP.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.HTTP.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hue.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hue.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.httpfs.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.httpfs.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hdfs.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hdfs.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.yarn.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.yarn.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
    <!-- 组内用户列表的类设定 -->
  </property>
  <property>
    <name>hadoop.security.instrumentation.requires.admin</name>
    <value>false</value>
    <!-- (acl access control list)-->
    <!-- Indicates if administrator ACLs are required to access instrumentation servlets (JMX, METRICS, CONF, STACKS). -->
  </property>
  <property>
    <name>net.topology.script.file.name</name>
    <value>/etc/hadoop/conf.cloudera.yarn/topology.py</value>
    <!-- 网络拓扑结构  指定一个把DNS名称翻译成网络图谱结构的脚本 -->
    <!-- The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output. -->
  </property>
  <property>
    <name>io.file.buffer.size</name>
    <value>65536</value>
  </property>
  <property>
    <name>hadoop.ssl.enabled</name>
    <value>false</value>
    <!-- secure sockets layer 安全套接层  -->
    <!-- ssl标准化之后名称改为TLS transport layer security -->
    <!-- 中文: 传输层安全协议 -->
  </property>
  <property>
    <name>hadoop.ssl.require.client.cert</name>
    <value>false</value>
    <final>true</final>
    <!-- 是否需要客户端证书 -->
  </property>
  <property>
    <name>hadoop.ssl.keystores.factory.class</name>
    <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
    <final>true</final>
    <!-- 检索证书的 factory.class -->
  </property>
  <property>
    <name>hadoop.ssl.server.conf</name>
    <value>ssl-server.xml</value>
    <final>true</final>
  </property>
  <property>
    <name>hadoop.ssl.client.conf</name>
    <value>ssl-client.xml</value>
    <final>true</final>
  </property>
</configuration>
