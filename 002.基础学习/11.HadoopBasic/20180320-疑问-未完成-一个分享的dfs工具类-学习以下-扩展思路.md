## hadoop的dfs工具类一个【原创】

　　开始没搞定插件问题，就弄了个dsf操作类，后面搞定了插件问题，这玩意也就聊胜于无了，还是丢这里算了。

　　首先是一个配置，ztool.hadoop.properties

```note
hadoop.home.dir=G:/hadoop/hadoop-2.4.1
hadoop.user.name=hadoop

hadoop.server.ip=192.168.117.128
hadoop.server.hdfs.port=9000
```

　　前面两个属性后面代码会有说明的。

　　属性文件的读取，方法多了，一般用commons-configuration包，我是自己把这个再整了一次，加了些自动处理，这个代码中可以无视，直接把代码中的那部分改成普通引用就好了。

　　logger部分，用了logback，也是处理了一下，处理了其在linux下会莫名其妙找不到配置文件的问题。这里就不放出代码了，直接把代码中的那部分改成普通引用就好了，我就不改了。

　　工具类代码如下

```java
package com.cnblogs.zxub.hadoop.dfs;

import java.io.IOException;
import java.net.URI;

import org.apache.commons.configuration.PropertiesConfiguration;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.mapred.JobConf;
import org.slf4j.Logger;

import com.cnblogs.zxub.util.logger.ZLoggerFactory;
import com.cnblogs.zxub.util.properties.PropertiesLoader;

public class DfsUtil {
    // 可以看看这个日志内容
    private static final Logger logger = ZLoggerFactory
            .getLogger(DfsUtil.class);
    // 1. 获取 hdfs 配置
    private final PropertiesConfiguration props = PropertiesLoader
            .getConfiguration("ztool.hadoop");
    private Configuration config = null;
    private String hdfsPath = null;

    private String baseDir = null;
    // 下面是 工具包 - ！！！！！！！！！！！！！！！！！！！！！！！
    public DfsUtil(String hdfs, Configuration config) {
        // windows下设置HADOOP_HOME后，还可能找不到winutils.exe，直接自己写进去程序算了
        System.setProperty("hadoop.home.dir",
                this.props.getString("hadoop.home.dir"));
        // 设置与dfs服务通信的用户名，省得换当前用户名，也不改配置关闭权限控制了
        System.setProperty("HADOOP_USER_NAME",
                this.props.getString("hadoop.user.name"));
                // 如果没提供 hdfs，就通过参数文件自己拼接以下，如果有就直接用
        this.hdfsPath = (hdfs == null) ? "hdfs://"
                + this.props.getString("hadoop.server.ip") + ":"
                + this.props.getString("hadoop.server.hdfs.port") + "/" : hdfs;
        // job conf 这个 我目前没用到，因为都是用spark任务
        if (config == null) {
            JobConf conf = new JobConf(DfsUtil.class);
            conf.setJobName("HdfsDAO");
            config = conf;
        }
        this.config = config;
    }
    // 从配置文件 获取了 很多 配置参数后，给这些配置参数 设置 get 方法，让程序可以获取
    // todo ： 这个功能 可以添加到我的项目里面！！！！
    public DfsUtil(Configuration conf) {
        this(null, conf);
    }

    public DfsUtil() {
        this(null, null);
    }

    public String getBaseDir() {
        return this.baseDir;
    }

    public void setBaseDir(String baseDir) {
        this.baseDir = baseDir;
    }

    public String getHdfsPath() {
        return this.hdfsPath;
    }

    public Configuration getConfig() {
        return this.config;
    }
    // 处理 path， 让 path 标准化
    // hadoop  里面的 Path 对象，还要了解一下细节！！！！！！！！
    // ！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！
    private String standardPath(String path) {
        // 前 几个步骤 都是 处理 baseDir 的， baseDir应该是  /a/b   这种形式的
        if (this.baseDir == null) {
            this.baseDir = "/";
        }
        // path最前面 缺少 /  就补上
        if (this.baseDir.indexOf("/") != 0) {
            this.baseDir = "/" + this.baseDir;
        }
        // 如果 path最后 是个 / ，   为什么要这么做？？？？？
        // $ 是 正则表达式 匹配结尾的内容
        // todo ： ask for help
        // solved   这里面的  replaceFirst("/$","") 
        //                     1. String 类 里面 支持regex 的方法有两个
                          //        1） replaceFirst()
                          //        2） replaceAll()
                          //   2. 这里没有什么好方法，只能用 replaceFirst +  正则
                          //        "/$" 这个正则表示，匹配最后一个 /
        if (this.baseDir.lastIndexOf("/") == this.baseDir.length() - 1) {
            this.baseDir = this.baseDir.replaceFirst("/$", "");
        }


        // path 应该也是 以 / 开头
        if (path.indexOf("/") != 0) {
            path = "/" + path;
        }
        // 完整的path 
        path = this.baseDir + path;
        // 同样是 去处最后一个  /
        if (path.lastIndexOf("/") == path.length() - 1) {
            path = path.replaceFirst("/$", "");
        }
        if (path.isEmpty()) {
            path = "/";
        }
        return path;
    }

    public void ll(String folder) throws IOException {
        folder = this.standardPath(folder);
        Path path = new Path(folder);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        FileStatus[] list = fs.listStatus(path);
        System.out.println("ll: " + folder);
        for (FileStatus f : list) {
            System.out.printf("name: %s, folder: %s, size: %d\n", f.getPath(),
                    f.isDirectory(), f.getLen());
        }
        fs.close();
    }

    public void mkdirs(String folder) throws IOException {
        folder = this.standardPath(folder);
        Path path = new Path(folder);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        if (!fs.exists(path)) {
            fs.mkdirs(path);
            logger.info("create: {}.", folder);
        } else {
            logger.warn("folder [{}] already exists, mkdirs failed.", folder);
        }
        fs.close();
    }

    public void rm(String file) throws IOException {
        file = this.standardPath(file);
        Path path = new Path(file);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        fs.deleteOnExit(path);
        logger.info("delete: {}.", file);
        fs.close();
    }

    public void newFile(String file, String content) throws IOException {
        file = this.standardPath(file);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        byte[] buff = content.getBytes();
        FSDataOutputStream os = null;
        try {
            os = fs.create(new Path(file));
            os.write(buff, 0, buff.length);
            logger.info("create: {}.", file);
        } finally {
            if (os != null) {
                os.close();
            }
        }
        fs.close();
    }

    public void scp(String local, String remote) throws IOException {
        remote = this.standardPath(remote);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        fs.copyFromLocalFile(new Path(local), new Path(remote));
        logger.info("copy: from [{}] to [{}]", local, remote);
        fs.close();
    }

    public void download(String remote, String local) throws IOException {
        remote = this.standardPath(remote);
        Path path = new Path(remote);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        fs.copyToLocalFile(path, new Path(local));
        logger.info("download: from [{}] to [{}]", remote, local);
        fs.close();
    }

    public void cat(String remote) throws IOException {
        remote = this.standardPath(remote);
        Path path = new Path(remote);
        FileSystem fs = FileSystem.get(URI.create(this.getHdfsPath()),
                this.getConfig());
        FSDataInputStream fsdis = null;
        System.out.println("cat: " + remote);
        try {
            fsdis = fs.open(path);
            IOUtils.copyBytes(fsdis, System.out, 4096, false);
        } finally {
            IOUtils.closeStream(fsdis);
            fs.close();
        }
    }

    public static void main(String[] args) throws IOException {
        DfsUtil hdfs = new DfsUtil();
        // hdfs.setBaseDir("/test");
        // hdfs.mkdirs("/debug_in");
        // hdfs.newFile("/test.txt", "测试");
        // hdfs.rm("/test.txt");
        // hdfs.rm("/test");
        // hdfs.scp("c:/q.txt", "/");
        hdfs.ll("/");
        // hdfs.download("/test.txt", "c:/t.txt");
        // hdfs.cat("q.txt");
        // hdfs.scp("c:/din/f1.txt", "debug_in");
        // hdfs.scp("c:/din/f2.txt", "debug_in");
    }
}
```