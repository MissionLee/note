#

```scala
  /**
   *  Return a new RDD by first applying a function to all elements of this
   *  RDD, and then flattening the results.
   */
  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))
  }
```
方法解析
- U:ClassTag [ClassTag](../../相关内容/03.classTag.md)
- TraversableOnce[U] scala collection 下的一个trait :A template trait for collections which can be traversed either once only or one or more times.  
  - 我翻看了以下collection的源码， Iterable 是继承了这个类的，也就是所有的scala集合类型应该都符合要求，这里用这个特质代表泛型，应该是强调这个collection是可以1次或多次迭代的（lms： 20180402 推测）
  




源码注释：
  - 1.对RDD所有内容执行这个操作
  - 然后摊平结果

原本最经典的例子就是 WordCount使用flatMap 计算每个单词的数量，这里我做一个类似的

```scala
package basic

import org.apache.spark.sql.SparkSession
import shapeless.ops.nat.GT.>

import scala.collection.immutable.HashSet

/**
  *
  */
object TestFlatMap {
  val ss = SparkSession.builder().master("local").appName("1").getOrCreate()
  val sc = ss.sparkContext
  val rdd = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkbasic.txt")
  sc.setLogLevel("error")

  def main(args: Array[String]): Unit = {
    rdd.foreach(println)
    val flatrdd = rdd.flatMap(line=>line.split(",",-1))
    // String.split 返回的是个 String[]

    println("------------------")
    flatrdd.foreach(println)
    println("-------------------")

    // 这里定义一个 返回 Set 的函数
    def myfuc(line:String ):Set[String]={
      val list0 = line.split(",",-1)
      var hset = new HashSet[String]
      hset =hset + list0(0)
      hset =hset + list0(1)
      hset
    }
    val flatrdd2 = rdd.flatMap(myfuc)
    flatrdd2.foreach(println)
  }
}
```

控制台打印结果
```note
1,a,c,b
2,w,gd,h
3,h,r,x
4,6,s,b
5,h,d,o
6,q,w,e
------------------
1
a
c
b
2
w
gd
h
3
h
r
x
4
6
s
b
5
h
d
o
6
q
w
e
-------------------
a
1
2
w
h
3
4
6
5
h
6
q
```