```scala
  /**
   * Reduces the elements of this RDD in a multi-level tree pattern.
   *
   * @param depth suggested depth of the tree (default: 2)
   * @see [[org.apache.spark.rdd.RDD#reduce]]
   */
  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope {
    require(depth >= 1, s"Depth must be greater than or equal to 1 but got $depth.")
    val cleanF = context.clean(f)
    val reducePartition: Iterator[T] => Option[T] = iter => {
      if (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } else {
        None
      }
    }
    val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it)))
    val op: (Option[T], Option[T]) => Option[T] = (c, x) => {
      if (c.isDefined && x.isDefined) {
        Some(cleanF(c.get, x.get))
      } else if (c.isDefined) {
        c
      } else if (x.isDefined) {
        x
      } else {
        None
      }
    }
    partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth)
      .getOrElse(throw new UnsupportedOperationException("empty collection"))
  }
```

- 从作用上看 reduce 与 tree reduce 差不多

```scala
package basic

import org.apache.spark.sql.SparkSession

/**
  * 
  */
object TestTreeRedcue {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  def main(args: Array[String]): Unit = {

    println("----------reduce-----------")
    val c = sc.parallelize(1 to 10)
    val c2 = c.reduce(_+_)
    println(c2)
    println("-----------tree redcue-------")
    val c3 = c.treeReduce(_+_);
    println(c3)
  }
}

```

结果

```scala
----------reduce-----------
55
-----------tree redcue-------
55
```

区别（攻略文档中看到的）
- TreeReduce 应用与单个reduce操作开销较大的情况，会针对每个分区现行计算，然后聚合得到最终结果。
- 实际应用中可以代替 reduce