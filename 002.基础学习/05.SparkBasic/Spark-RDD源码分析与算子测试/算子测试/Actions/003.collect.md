# 

```scala
  /**
   * Return an array that contains all of the elements in this RDD.
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   */
  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }

    /**
   * Return an RDD that contains all matching values by applying `f`.
   */
  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    filter(cleanF.isDefinedAt).map(cleanF)
  }
```

返回一个包含RDD中所有的元素的数组

注意： 因为所有数据会在这个步骤装载到内存中，我们希望这个数组不要太大

>基础方法测试

```scala
object TestCollect {
  val ss = SparkSession.builder().master("local").appName("basic").getOrCreate()
  val sc = ss.sparkContext
  sc.setLogLevel("error")
  val a1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip1.txt")
  val a2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip2.txt")
  val b1 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip3.txt")
  val b2 = sc.textFile("/home/missingli/IdeaProjects/SparkLearn/src/main/resources/sparkzip4.txt")

  def main(args: Array[String]): Unit = {
    val rdd = a1++a2++b1++b2
    val array = rdd.collect()
    for(str<-array){
      println(str)
    }

  }
}
```

输出

```note
A1
A2
A3
A4
A5
A6
B1
B2
B3
B4
B5
B6
```

>带过滤的方法测试

在源代码中可以看到，collect实际上把我们提供的函数传-一个标准偏函数-递给了 filter

filter实际上是要求对元素进行判断并返回布尔值

