```scala

  /**
   * Return the number of elements in the RDD.
   */
  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum

  /**
   * Approximate version of count() that returns a potentially incomplete result
   * within a timeout, even if not all tasks have finished.
   *
   * The confidence is the probability that the error bounds of the result will
   * contain the true value. That is, if countApprox were called repeatedly
   * with confidence 0.9, we would expect 90% of the results to contain the
   * true count. The confidence must be in the range [0,1] or an exception will
   * be thrown.
   *
   * @param timeout maximum time to wait for the job, in milliseconds
   * @param confidence the desired statistical confidence in the result
   * @return a potentially incomplete result, with error bounds
   */
  def countApprox(
      timeout: Long,
      confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope {
    require(0.0 <= confidence && confidence <= 1.0, s"confidence ($confidence) must be in [0,1]")
    val countElements: (TaskContext, Iterator[T]) => Long = { (ctx, iter) =>
      var result = 0L
      while (iter.hasNext) {
        result += 1L
        iter.next()
      }
      result
    }
    val evaluator = new CountEvaluator(partitions.length, confidence)
    sc.runApproximateJob(this, countElements, evaluator, timeout)
  }
```
这里的这个方法的返回值挺有意思

[PartialResult[BoundedDouble]](../../相关内容/07.ParticalResult.md)

```scala
  /**
   * Return the count of each unique value in this RDD as a local map of (value, count) pairs.
   *
   * @note This method should only be used if the resulting map is expected to be small, as
   * the whole thing is loaded into the driver's memory.
   * To handle very large results, consider using
   *
   * {{{
   * rdd.map(x => (x, 1L)).reduceByKey(_ + _)
   * }}}
   *
   * , which returns an RDD[T, Long] instead of a map.
   */
  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope {
    map(value => (value, null)).countByKey()
  }

  /**
   * Approximate version of countByValue().
   *
   * @param timeout maximum time to wait for the job, in milliseconds
   * @param confidence the desired statistical confidence in the result
   * @return a potentially incomplete result, with error bounds
   */
  def countByValueApprox(timeout: Long, confidence: Double = 0.95)
      (implicit ord: Ordering[T] = null)
      : PartialResult[Map[T, BoundedDouble]] = withScope {
    require(0.0 <= confidence && confidence <= 1.0, s"confidence ($confidence) must be in [0,1]")
    if (elementClassTag.runtimeClass.isArray) {
      throw new SparkException("countByValueApprox() does not support arrays")
    }
    val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (ctx, iter) =>
      val map = new OpenHashMap[T, Long]
      iter.foreach {
        t => map.changeValue(t, 1L, _ + 1L)
      }
      map
    }
    val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence)
    sc.runApproximateJob(this, countPartition, evaluator, timeout)
  }

  /**
   * Return approximate number of distinct elements in the RDD.
   *
   * The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
   * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
   * <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
   *
   * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater
   * than `p`) would trigger sparse representation of registers, which may reduce the memory
   * consumption and increase accuracy when the cardinality is small.
   *
   * @param p The precision value for the normal set.
   *          `p` must be a value between 4 and `sp` if `sp` is not zero (32 max).
   * @param sp The precision value for the sparse set, between 0 and 32.
   *           If `sp` equals 0, the sparse representation is skipped.
   */
  def countApproxDistinct(p: Int, sp: Int): Long = withScope {
    require(p >= 4, s"p ($p) must be >= 4")
    require(sp <= 32, s"sp ($sp) must be <= 32")
    require(sp == 0 || p <= sp, s"p ($p) cannot be greater than sp ($sp)")
    val zeroCounter = new HyperLogLogPlus(p, sp)
    aggregate(zeroCounter)(
      (hll: HyperLogLogPlus, v: T) => {
        hll.offer(v)
        hll
      },
      (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => {
        h1.addAll(h2)
        h1
      }).cardinality()
  }

  /**
   * Return approximate number of distinct elements in the RDD.
   *
   * The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
   * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
   * <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
   *
   * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
   *                   It must be greater than 0.000017.
   */
  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope {
    require(relativeSD > 0.000017, s"accuracy ($relativeSD) must be greater than 0.000017")
    val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt
    countApproxDistinct(if (p < 4) 4 else p, 0)
  }

```

- count 的基本作用是 计算rdd中的元素个数
  - countApprox 估算元素个数，给定一个限定时间 timeout，时间结束后直接返回结果（即使此时还未计算完成）
    - 参数1 ： 毫秒为的限定时间
    - 参数2 ： 希望的统计可信度
    - 返回： 一个带有一定范围误差的结果
    - 如果在限定时间内完成，会返回准确的结果
    - 返回的是一个区间，意思是 最终的count值，估计在这个结果区间内，可信度为给定可信度
    - `没有作出能反映更多细节的DEMO`
  - countByValue
    - 返回一个，计算每个元素分别出现的此书，返回 健值对形式 
    ```scala
    val rdd = sc.parallelize(Range(1,10,1))
    val rdd2 = sc.parallelize(Range(1,10,2))
    val rdd3 = rdd++rdd2;
    rdd3.countByValue().foreach(println)
    // 结果如下
    (5,2)
    (1,2)
    (6,1)
    (9,2)
    (2,1)
    (7,2)
    (3,2)
    (8,1)
    (4,1)
    ```
  - countByValueApprox
    - countByValue对应的估算版本
  - countApproxDistinct  - 计算单一元素（不与其他重复的元素）在rdd中大概出现的次数
    - 单一元素： 例如 List(1,2,3,4,4) 那么 1，2，3 会被这个算子计算到
    - 有两个重载
      - 提供两个精度值的版本
        - 参数1 p， 普通set的精度值，介于 4 和参数2之间（如果参数2不为0）
        - 参数2 sp，稀疏set的精度值，在0～32 之间。如果sp为0，会跳过 稀疏表示
      - 提供 一个相对精度
    - 这两者对应数学上不同的算法，暂时没有深入研究
    ```scala
        val rdd = sc.parallelize(Range(1,1000))
    val x =rdd.countApproxDistinct(0.1)
    val y = rdd.countApproxDistinct(0.2)
    val z = rdd.countApproxDistinct(0.6)
    val num = rdd.countApproxDistinct(1)
    val num2 = rdd.countApproxDistinct(100)

    println(x) //1194
    println(y) //1476
    println(z) //1213
    println(num) //1213
    println(num2) //1213
    ```
    - 测试了其中第二个版本，但是测试量很小，当前例子，在给定参数比较小的时候，可以得到较为精确的值（0.01时为1000）