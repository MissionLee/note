# Data Types and File Formats

Hive supports many of the primitive data types you find in relational databases, as well as three collection data types that are rarely found in relational databases, for reasons we’ll discuss shortly.\
`Hive支持很多传统的数据类型,并且有三种在其他库中很少见的类型`

A related concern is how these types are represented in text files, as well as alternatives to text storage that address various performance and other concerns. A unique feature of Hive, compared to most databases, is that it provides great flexibility in how data is encoded in files. Most databases take total control of the data, both how it is persisted to disk and its life cycle. By letting you control all these aspects, Hive makes it easier to manage and process data with a variety of tools.\
`与之相关的是,这些类型如何在文本文件中呈现,以及如何代替文本存储已解决性能问题或者其他一些问题`\
`Hive独有的特质,它在文件的编码(不是utf8这个编码,是编辑组织文档内文本的形式这种意思)上有很大的灵活性,多数情况数据库掌控编码之类的.不过,hive重这些都是可控的(我现在看到的,比如 指定字段之间 用 tab作为分割,这种就是可以指定的),这就使得hive可以和很多工具配合使用`

## Primitive Data Types

Hive supports several sizes of integer and floating-point types, a Boolean type, and character strings of arbitrary length. Hive v0.8.0 added types for timestamps and binary fields.

Table 3-1 lists the primitive types supported by Hive.

`Table 3-1. Primitive data types`
|Type |Size |Literal syntax examples
|-|-|-
|TINYINT |1 byte signed integer. |20
|SMALLINT |2 byte signed integer. |20
|INT |4 byte signed integer. |20
|BIGINT |8 byte signed integer. |20
|BOOLEAN |Boolean true or false. |TRUE
|FLOAT |Single precision floating point. |3.14159
|DOUBLE |Double precision floating point. |3.14159
|STRING |Sequence of characters. The character set can be specified. Single or double quotes can be used. |'Now is the time', "for all good men"
|TIMESTAMP (v0.8.0+) |Integer, float, or string. |1327882394 (Unix epoch seconds), 1327882394.123456789 (Unix epoch seconds plus nanoseconds), and '2012-02-03 |12:34:56.123456789' (JDBCcompliant java.sql.Timestamp format)
|BINARY (v0.8.0+) |Array of bytes. |See discussion below

As for other SQL dialects, the case of these names is ignored.

It’s useful to remember that each of these types is implemented in Java, so the particular behavior details will be exactly what you would expect from the corresponding Java types. For example, STRING is implemented by the Java String, FLOAT is implemented by Java float, etc.\
`所有类型都是来自于Java,所以从java的角度考虑这些数据类型完全OK`

Note that `Hive does not support “character arrays” (strings) with maximum-allowed lengths,` as is common in other SQL dialects. Relational databases offer this feature as a performance optimization; fixed-length records are easier to index, scan, etc. In the “looser” world in which Hive lives, where it may not own the data files and has to be flexible on file format, Hive relies on the presence of delimiters to separate fields. Also, Hadoop and Hive emphasize optimizing disk reading and writing performance, where fixing the lengths of column values is relatively unimportant.\
`Hive没有可设置上限的字符数组(比如char(20)这样的)`\
`定长有利于检索之类的,但是在hive所处的条件=>有时候hive并没有它要管理的文件的所有权,hive需要依赖于指定分隔符来区分字段`\
`Hadoop和hive都是强调优化读写操作的,定长这种条件在这种情况下不重要`

Values of the new TIMESTAMP type can be integers, which are interpreted as seconds since the Unix epoch time (Midnight, January 1, 1970), floats, which are interpreted as seconds since the epoch time with nanosecond resolution (up to 9 decimal places), and strings, which are interpreted according to the JDBC date string format convention, YYYY-MM-DD hh:mm:ss.fffffffff.\
`时间戳可以是integer=>秒数,floats=>毫秒数,strings=>JDBC date`

TIMESTAMPS are interpreted as UTC times. Built-in functions for conversion to and from timezones are provided by Hive, to_utc_timestamp and from_utc_timestamp, respectively (see Chapter 13 for more details).\
`时间戳被解释为UTC事件,Hive中自动将他们转换为时区时间=>13章里面有详解`

The `BINARY type` is similar to the `VARBINARY type` found in many relational databases. It’s not like a BLOB type, since BINARY columns are stored within the record, not separately like BLOBs. BINARY can be used as a way of including arbitrary bytes in a record and preventing Hive from attempting to parse them as numbers, strings, etc.\
`二进制类型与传统看的VARBINARY类型相似,不过它并不是二进制大文件,因为其内容还是存在记录中的,不像二进制文件单独存放.二进制类型可以记录人以字节,同时防止hive尝试将这些内容用数字,字符等方式解析`

Note that you don’t need BINARY if your goal is to ignore the tail end of each record. If a table schema specifies three columns and the data files contain five values for each record, the last two will be ignored by Hive.\
`不过你不需要使用BINARY类型来达到忽略每条记录尾部内容的目的,因为尾部多出来的内容会被自动忽略`

What if you run a query that wants to compare a float column to a double column or compare a value of one integer type with a value of a different integer type? Hive will implicitly cast any integer to the larger of the two integer types, cast FLOAT to DOUBLE, and cast any integer value to DOUBLE, as needed, so it is comparing identical types.\`不同数字类型之间比较的时候,hive会进行隐式转换`

What if you run a query that wants to interpret a string column as a number? You can explicitly cast one type to another as in the following example, where s is a string column that holds a value representing an integer:
... cast(s AS INT) ...;
(To be clear, the AS INT are keywords, so lowercase would be fine.)\
`如果想把字符串和数字比较,可以使用 cast(s AS INT)这种方法`

We’ll discuss data conversions in more depth in “Casting” on page 109.

## Collection Data Types

Hive supports columns that are structs, maps, and arrays. Note that the literal syntax examples in Table 3-2 are actually calls to built-in functions.

`Table 3-2. Collection data types`

|Type |Description |Literal syntax examples
|-|-|-
|STRUCT |Analogous to a C struct or an “object.” Fields can be accessed using the “dot” notation. For example, if a column name is of type STRUCT {first STRING; last STRING}, then the first name field can be referenced using name.first. |struct('John', 'Doe')
|MAP |A collection of key-value tuples, where the fields are accessed using array notation (e.g., ['key']). For example, if a column name is of type MAP with key→value pairs 'first'→'John' and 'last'→'Doe', then the last name can be referenced using name['last']. |map('first', 'John', 'last', 'Doe')
|ARRAY |Ordered sequences of the same type that are indexable using zero-based integers. For example, if a column name is of type ARRAY of strings with the value ['John', 'Doe'], then the second element can be referenced using name[1]. |array('John', 'Doe')

As for simple types, the case of the type name is ignored.

Most relational databases don’t support such collection types, because using them tends to break normal form. For example, in traditional data models, structs might be captured in separate tables, with foreign key relations between the tables, as appropriate.\
`结构数据会破坏标准模式=>关系模式`

A practical problem with breaking normal form is the `greater risk of data duplication`, leading to unnecessary disk space consumption and potential data inconsistencies, as duplicate copies can grow out of sync as changes are made.\
`破坏标准模式=>数据冗余过多`

However, in Big Data systems, a benefit of sacrificing normal form is higher processing throughput. Scanning data off hard disks with minimal “head seeks” is essential when processing terabytes to petabytes of data. Embedding collections in records makes retrieval faster with minimal seeks. Navigating each foreign key relationship requires seeking across the disk, with significant performance overhead.\
`在大数据系统中,通常,存取速度比存储空间优先级高,标准模式里面那些关联操作,在大数据环境中,甚至需要跨机器查询才能完成`

```note
Hive doesn’t have the concept of keys. However, you can index tables, as we’ll see in Chapter 7.
```

Here is a table declaration that demonstrates how to use these types, an employees table in a fictitious Human Resources application:

```sql
CREATE TABLE employees (
 name STRING,
 salary FLOAT,
 subordinates ARRAY<STRING>,
 deductions MAP<STRING, FLOAT>,
 address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>);
```

The name is a simple string and for most employees, a float is large enough for the salary. The list of subordinates(下属) is an array of string values, where we treat the name as a “primary key,” so each element in subordinates would reference another record in the table. Employees without subordinates would have an empty array. In a traditional model, the relationship would go the other way, from an employee to his or her manager. We’re not arguing that our model is better for Hive; it’s just a contrived example to illustrate the use of arrays.\
`这里只是举例说明Array的用法,并不是这种方式更适合`

The deductions(扣除) is a map that holds a key-value pair for every deduction that will be subtracted from the employee’s salary when paychecks are produced. The key is the name of the deduction (e.g., “Federal Taxes”), and the key would either be a percentage value or an absolute number. In a traditional data model, there might be separate tables for deduction type (each key in our map), where the rows contain particular deduction values and a foreign key pointing back to the corresponding employee record. Finally, the home address of each employee is represented as a struct, where each field is named and has a particular type.

Note that Java syntax conventions for generics are followed for the collection types. For example, MAP<STRING, FLOAT> means that every key in the map will be of type STRING and every value will be of type FLOAT. For an ARRAY<STRING>, every item in the array will be a STRING. STRUCTs can mix different types, but the locations are fixed to the declared position in the STRUCT.

## Text File Encoding of Data Values

Let’s begin our exploration of file formats by looking at the simplest example, text files.

You are no doubt familiar with text files delimited with commas or tabs, the so-called `comma-separated values (CSVs)` or `tab-separated values (TSVs)`, respectively. Hive can use those formats if you want and we’ll show you how shortly. However, there is a drawback to both formats; you have to be careful about commas or tabs embedded in text and not intended as field or column delimiters. For this reason, Hive uses various control characters by default, which are less likely to appear in value strings. Hive uses the term field when overriding the default delimiter, as we’ll see shortly. They are listed in Table 3-3.

`Table 3-3. Hive’s default record and field delimiters\Hive中默认的记录和字段分隔符`
|Delimiter |Description
|-|-
|\n |For text files, each line is a record, so the line feed character separates records.`文本文档中,每一行是一条记录`
|^A (“control” A) |Separates all fields (columns). Written using the octal code \001 when explicitly specified in CREATE TABLE statements.`用于分隔字段,在create table里面使用把进制编码 \001 表示`
|^B |Separate the elements in an ARRAY or STRUCT, or the key-value pairs in a MAP. Written using the octal code \002 when explicitly specified in CREATE TABLE statements.`用于分隔Array或者struct中的元素,或用于Map中键值对之间的分割,create table语句中使用把进制编码 \002`
|^C |Separate the key from the corresponding value in MAP key-value pairs. Written using the octal code \003 when explicitly specified in CREATE TABLE statements.`Map中键和值之间的分分隔,create table 语句中使用八进制 \003 表示`

Records for the employees table declared in the previous section would look like the following example, where we use ^A, etc., to represent the field delimiters. A text editor like Emacs will show the delimiters this way. Note that the lines have been wrapped in the example because they are too long for the printed page. To clearly indicate the division between records, we have added blank lines between them that would not appear in the file:

John Doe^A100000.0^AMary Smith^BTodd Jones^AFederal Taxes^C.2^BState
Taxes^C.05^BInsurance^C.1^A1 Michigan Ave.^BChicago^BIL^B60600

Mary Smith^A80000.0^ABill King^AFederal Taxes^C.2^BState Taxes^C.
05^BInsurance^C.1^A100 Ontario St.^BChicago^BIL^B60601

Todd Jones^A70000.0^AFederal Taxes^C.15^BState Taxes^C.03^BInsurance^C.
1^A200 Chicago Ave.^BOak Park^BIL^B60700

Bill King^A60000.0^AFederal Taxes^C.15^BState Taxes^C.03^BInsurance^C.
1^A300 Obscure Dr.^BObscuria^BIL^B60100

This is a little hard to read, but you would normally let Hive do that for you, of course. Let’s walk through the first line to understand the structure. First, here is what it would look like in JavaScript Object Notation (JSON), where we have also inserted the names from the table schema:

```json
{
 "name": "John Doe",
 "salary": 100000.0,
 "subordinates": ["Mary Smith", "Todd Jones"],
 "deductions": {
    "Federal Taxes": .2,
    "State Taxes": .05,
    "Insurance": .1
 },
 "address": {
    "street": "1 Michigan Ave.",
    "city": "Chicago",
    "state": "IL",
    "zip": 60600
 }
}
```

You’ll note that maps and structs are effectively the same thing in JSON.

Now, here’s how the first line of the text file breaks down:

- • `John Doe` is the `name`.
- • `100000.0` is the `salary`.
- • `Mary Smith^BTodd Jones` are the `subordinates` “Mary Smith” and “Todd Jones.”
- • `Federal Taxes^C.2^BState Taxes^C.05^BInsurance^C.1` are the deductions, where 20% is deducted for “Federal Taxes,” 5% is deducted for “State Taxes,” and 10% is deducted for “Insurance.”
- • `1 Michigan Ave.^BChicago^BIL^B60600` is the address, “1 Michigan Ave., Chicago, 60600.”

You can override these default delimiters. This might be necessary if another application writes the data using a different convention. Here is the same table declaration again, this time with all the format defaults explicitly specified:\
`用户可以使用自定义的分隔符-> 因为这些数据可能也被其他应用使用,要配合他们的要求`

```sql
CREATE TABLE employees (
 name STRING,
 salary FLOAT,
 subordinates ARRAY<STRING>,
 deductions MAP<STRING, FLOAT>,
 address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\001'
COLLECTION ITEMS TERMINATED BY '\002'
MAP KEYS TERMINATED BY '\003'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;
```

The `ROW FORMAT DELIMITED` sequence of keywords must appear before any of the other clauses, with the exception of the `STORED AS …` clause.

The character `\001` is the octal code for `^A`. The clause R`OW FORMAT DELIMITED FIELDS TERMINATED BY '\001'` means that Hive will use the ^A character to separate fields.

Similarly, the character `\002` is the octal code for `^B`. The clause `ROW FORMAT DELIMITED COLLECTION ITEMS TERMINATED BY '\002'` means that Hive will use the ^B character to separate collection items.

Finally, the character `\003` is the octal code for `^C`. The clause `ROW FORMAT DELIMITED MAP KEYS TERMINATED BY '\003'` means that Hive will use the ^C character to separate map keys from values.

The clause `LINES TERMINATED BY '…'` and `STORED AS …` do not require the `ROW FORMAT DELIMITED` keywords.\
`LINES,STORED AS两个不需要ROW FORMAT DELIMITED关键词`

Actually, it turns out that Hive does not currently support any character for LINES TERMINATED BY … other than '\n'. So this clause has limited utility today.\
`行分隔符目前只支持 \n所以也没得选`

You can override the field, collection, and key-value separators and still use the default text file format, so the clause STORED AS TEXTFILE is rarely used. For most of this book, we will use the default TEXTFILE file format.

There are other file format options, but we’ll defer discussing them until Chapter 15. A related issue is compression of files, which we’ll discuss in Chapter 11.

So, while you can specify all these clauses explicitly, using the default separators most of the time, you normally only provide the clauses for explicit overrides.

```note
These specifications only affect what Hive expects to see when it reads files. Except in a few limited cases, it’s up to you to write the data files in the correct format.
```

For example, here is a table definition where the data will contain comma-delimited fields.

```sql
CREATE TABLE some_data (
 first FLOAT,
 second FLOAT,
 third FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
```

Use '\t' for tab-delimited fields.

```note
This example does not properly handle the general case of files in CSV (comma-separated values) and TSV (tab-separated values) formats. They can include a header row with column names and column string values might be quoted and they might contain embedded commas or tabs, respectively. See Chapter 15 for details on handling these file types more generally.
```

This powerful customization feature makes it much easier to use Hive with files created by other tools and various ETL (extract, transform, and load) processes.

## Schema on Read \ 读时模式 =>读取数据的时候,对数据进行验证,解析

When you write data to a traditional database, either through loading external data, writing the output of a query, doing UPDATE statements, etc., the database has total control over the storage. The database is the “gatekeeper.” An important implication of this control is that the database can enforce the schema as data is written. This is called schema on write.

Hive has no such control over the underlying storage. There are many ways to create, modify, and even damage the data that Hive will query. Therefore, Hive can only enforce queries on read. This is called schema on read.

So what if the schema doesn’t match the file contents? Hive does the best that it can to read the data. You will get lots of null values if there aren’t enough fields in each record to match the schema. If some fields are numbers and Hive encounters nonnumeric strings, it will return nulls for those fields. Above all else, Hive tries to recover from all errors as best it can.