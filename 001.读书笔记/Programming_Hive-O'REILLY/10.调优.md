# Tunning

HiveQL is a declarative language where users issue declarative queries and Hive figures out how to translate them into MapReduce jobs. Most of the time, you don’t need to understand how Hive works, freeing you to focus on the problem at hand. While the sophisticated process of query parsing, planning, optimization, and execution is the result of many years of hard engineering work by the Hive team, most of the time you can remain oblivious to it.

However, as you become more experienced with Hive, learning about the theory behind Hive, and the low-level implementation details, will let you use Hive more effectively, especially where performance optimizations are concerned.

This chapter covers several different topics related to tuning Hive performance. Some tuning involves adjusting numeric configuration parameters (“turning the knobs”), while other tuning steps involve enabling or disabling specific features.

## Using EXPLAIN

The first step to learning how Hive works (after reading this book…) is to use the `EXPLAIN feature` to learn `how Hive translates queries into MapReduce jobs.`

Consider the following example:

```sql
hive> DESCRIBE onecol;
number int
hive> SELECT * FROM onecol;
5
5
4
hive> SELECT SUM(number) FROM onecol;
14
```

Now, put the EXPLAIN keyword in front of the last query to see the query plan and other information. The query will not be executed.

```sql
hive> EXPLAIN SELECT SUM(number) FROM oneco;
```

The output requires some explaining and practice to understand.

First, the `abstract syntax tree is printed`. This shows how Hive parsed the query into `tokens`[符号] and `literals`[字面值], as part of the first step in turning the query into the ultimate result:

```txt
ABSTRACT SYNTAX TREE:
(TOK_QUERY
    (TOK_FROM (TOK_TABREF (TOK_TABNAME onecol)))
    (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))
    (TOK_SELECT
        (TOK_SELEXPR
            (TOK_FUNCTION sum (TOK_TABLE_OR_COL number))))))
```

(The indentation of the actual output was changed to fit the page.)

For those not familiar with parsers and tokenizers, this can look overwhelming. However, even if you are a novice in this area, you can study the output to get a sense for what Hive is doing with the SQL statement. (As a first step, ignore the TOK_ prefixes.)\
`忽略掉TOK前缀`

Even though our query will write its output to the console, Hive will actually write the output to a temporary file first, as shown by this part of the output:\
`尽管查询会将输出写入控制台,单Hive实际上会先将输出写入到一个临时的文件中`

'(TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE))'\
`------插入---------目标位置------目录------临时文件`

Next, we can see references to our column name number, our table name onecol, and the sum function.

A Hive job consists of one or more stages, with dependencies between different stages. As you might expect, more complex queries will usually involve more stages and more stages usually requires more processing time to complete.

A stage could be `a MapReduce job, a sampling stage, a merge stage, a limit stage, or a stage for some other task Hive needs to do`. By default, Hive executes these stages one at a time, although later we’ll discuss parallel execution in “Parallel Execution” on page 136.\
`一个Hive任务,可能包含一个或多个阶段,MapReduce,sampling,merge,limit或者其他`

Some stages will be short, like those that move files around. Other stages may also finish quickly if they have little data to process, even though they require a map or reduce task:

```txt
STAGE DEPENDENCIES:
 Stage-1 is a root stage
 Stage-0 is a root stage
```

The STAGE PLAN section is verbose and complex. Stage-1 is the bulk of the processing for this job and happens via a MapReduce job. A TableScan takes the input of the table and produces a single output column number. The Group By Operator applies the sum(number) and produces an output column _col0 (a synthesized name for an anonymous result). All this is happening on the map side of the job, under the Map Operator

```txt
Tree:
STAGE PLANS:
    Stage: Stage-1
        Map Reduce
            Alias -> Map Operator Tree:
                onecol
                    TableScan
                        alias: onecol
                        Select Operator
                            expressions:
                                expr: number
                                type: int
                            outputColumnNames: number
                            Group By Operator
                                aggregations:
                                    expr: sum(number)
                                bucketGroup: false
                                mode: hash
                                outputColumnNames: _col0
                                Reduce Output Operator
                                    sort order:
                                    tag: -1
                                    value expressions:
                                        expr: _col0
                                        type: bigint
```

On the reduce side, under the Reduce Operator Tree, we see the same Group by Opera tor but this time it is applying sum on _col0. Finally, in the reducer we see the File Output Operator, which shows that the output will be text, based on the string output format: HiveIgnoreKeyTextOutputFormat:

```txt
 Reduce Operator Tree:
 Group By Operator
 aggregations:
 expr: sum(VALUE._col0)
 bucketGroup: false
 mode: mergepartial
 outputColumnNames: _col0
 Select Operator
 expressions:
 expr: _col0
 type: bigint
 outputColumnNames: _col0
 File Output Operator
 compressed: false
 GlobalTableId: 0
 table:
 input format: org.apache.hadoop.mapred.TextInputFormat
 output format:
 org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Because this job has no LIMIT clause, Stage-0 is a no-op stage:
 Stage: Stage-0
 Fetch Operator
 limit: -1
```

Understanding the intricate details of how Hive parses and plans every query is not useful all of the time. However, it is a nice to have for analyzing complex or poorly performing queries, especially as we try various tuning steps. We can observe what effect these changes have at the “logical” level, in tandem with performance measurements.

## 下面是一个实例

```bash
0: jdbc:hive2://BigData-06:10000> EXPLAIN SELECT COUNT(*) FROM food;
INFO  : Compiling command(queryId=hive_20171123171111_fec8abdc-05f1-401c-9537-3d133003b6c1): EXPLAIN SELECT COUNT(*) FROM food
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20171123171111_fec8abdc-05f1-401c-9537-3d133003b6c1); Time taken: 0.6 seconds
INFO  : Executing command(queryId=hive_20171123171111_fec8abdc-05f1-401c-9537-3d133003b6c1): EXPLAIN SELECT COUNT(*) FROM food
INFO  : Starting task [Stage-2:EXPLAIN] in serial mode
INFO  : Completed executing command(queryId=hive_20171123171111_fec8abdc-05f1-401c-9537-3d133003b6c1); Time taken: 0.033 seconds
INFO  : OK
+----------------------------------------------------+--+
|                      Explain                       |
+----------------------------------------------------+--+
| STAGE DEPENDENCIES:                                |
|   Stage-1 is a root stage                          |
|   Stage-0 depends on stages: Stage-1               |
|                                                    |
| STAGE PLANS:                                       |
|   Stage: Stage-1                                   |
|     Spark                                          |
|       Edges:                                       |
|         Reducer 2 <- Map 1 (GROUP, 1)              |
##           Map把每条记录映射为[GROUP,1]键值对,然后 Reducer |
|       DagName: hive_20171123171111_fec8abdc-05f1-401c-9537-3d133003b6c1:1 |
|       Vertices:                                    |
|         Map 1                                      |
|             Map Operator Tree:                     |
|                 TableScan                          |
##                    扫描表
|                   alias: food                      |
|                     //food表
|                   Statistics: Num rows: -1 Data size: 23388 Basic stats: PARTIAL Column stats: COMPLETE |
|                   Select Operator                  |
##                     Select操作
|                     Statistics: Num rows: -1 Data size: 23388 Basic stats: PARTIAL Column stats: COMPLETE |
|                     Group By Operator              |
|                       aggregations: count()        |
##                      聚合方式:count()
|                       mode: hash                   |
|                       outputColumnNames: _col0   //聚合处的数据  |
|                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                       Reduce Output Operator       |
|                         sort order:                |
|                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                         value expressions: _col0 (type: bigint) |
|         Reducer 2                                  |
|             Reduce Operator Tree:                  |
|               Group By Operator                    |
|                 aggregations: count(VALUE._col0)   |
##                 局和操作:count(VALUE._col0)  把所有map的到的_col0 再count()?
|                 mode: mergepartial   //模式:              |
|                 outputColumnNames: _col0           |
|                 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                 File Output Operator               |
|                   compressed: false      //压缩:false          |
|                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                   table:                           |
|                       input format: org.apache.hadoop.mapred.TextInputFormat |
|                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                                                    |
|   Stage: Stage-0                                   |
|     Fetch Operator                                 |
|       limit: -1                                    |
|       Processor Tree:                              |
|         ListSink                                   |
|                                                    |
+----------------------------------------------------+--+
48 rows selected (0.768 seconds)
```

## 这一部分实际运行如下

```bash
SELECT COUNT(*) FROM food;
INFO  : Compiling command(queryId=hive_20171123172828_69db710f-1910-449f-b40b-a9093c9e40e8): SELECT COUNT(*) FROM food
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20171123172828_69db710f-1910-449f-b40b-a9093c9e40e8); Time taken: 0.257 seconds
INFO  : Executing command(queryId=hive_20171123172828_69db710f-1910-449f-b40b-a9093c9e40e8): SELECT COUNT(*) FROM food
INFO  : Query ID = hive_20171123172828_69db710f-1910-449f-b40b-a9093c9e40e8
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
###启动一个job
INFO  : Starting task [Stage-1:MAPRED] in serial mode
### 启动task: 第一步=> MAPERD
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : Starting Spark Job = 5a50e809-8bda-46a3-bc1c-096917c09460
### 启动Spark Job
INFO  : Running with YARN Application = application_1509607079777_0036
### 在YAEN App下运行
INFO  : Kill Command = /home/opt/cloudera/parcels/CDH-5.12.0-1.cdh5.12.0.p0.29/lib/hadoop/bin/yarn application -kill application_1509607079777_0036
INFO  :
Query Hive on Spark job[0] stages:
INFO  : 0
INFO  : 1
INFO  :
Status: Running (Hive on Spark job[0])
INFO  : Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
INFO  : 2017-11-23 17:28:56,783	Stage-0_0: 0(+1)/1	Stage-1_0: 0/1
INFO  : 2017-11-23 17:28:59,825	Stage-0_0: 0(+1)/1	Stage-1_0: 0/1
INFO  : 2017-11-23 17:29:00,831	Stage-0_0: 1/1 Finished	Stage-1_0: 0(+1)/1
INFO  : 2017-11-23 17:29:01,839	Stage-0_0: 1/1 Finished	Stage-1_0: 1/1 Finished
INFO  : Status: Finished successfully in 15.11 seconds
INFO  : =====Spark Job[5a50e809-8bda-46a3-bc1c-096917c09460] statistics=====
INFO  : HIVE
INFO  : 	CREATED_FILES: 1
INFO  : 	DESERIALIZE_ERRORS: 0
INFO  : 	RECORDS_OUT_INTERMEDIATE: 1
INFO  : 	RECORDS_IN: 59
INFO  : 	RECORDS_OUT_0: 1
INFO  : Spark Job[5a50e809-8bda-46a3-bc1c-096917c09460] Metrics
INFO  : 	ExecutorDeserializeTime: 2778
INFO  : 	ExecutorRunTime: 1813
INFO  : 	ResultSize: 3501
INFO  : 	JvmGCTime: 96
INFO  : 	ResultSerializationTime: 0
INFO  : 	MemoryBytesSpilled: 0
INFO  : 	DiskBytesSpilled: 0
INFO  : 	BytesRead: 28079
INFO  : 	RemoteBlocksFetched: 0
INFO  : 	LocalBlocksFetched: 1
INFO  : 	TotalBlocksFetched: 1
INFO  : 	FetchWaitTime: 0
INFO  : 	RemoteBytesRead: 0
INFO  : 	ShuffleBytesWritten: 38
INFO  : 	ShuffleWriteTime: 23055826
INFO  : Execution completed successfully
INFO  : Completed executing command(queryId=hive_20171123172828_69db710f-1910-449f-b40b-a9093c9e40e8); Time taken: 25.81 seconds
INFO  : OK
+------+--+
| _c0  |
+------+--+
| 59   |
+------+--+
1 row selected (26.27 seconds)
```

## EXPLAIN EXTENDED

Using EXPLAIN EXTENDED produces even more output. In an effort to “go green,” we won’t show the entire output, but we will show you the Reduce Operator Tree to demonstrate the different output:

```bash
 EXPLAIN EXTENDED SELECT COUNT(*) FROM food;
INFO  : Compiling command(queryId=hive_20171123175050_1043dcd7-7560-4509-903d-e425b9eb483b): EXPLAIN EXTENDED SELECT COUNT(*) FROM food
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20171123175050_1043dcd7-7560-4509-903d-e425b9eb483b); Time taken: 0.097 seconds
INFO  : Executing command(queryId=hive_20171123175050_1043dcd7-7560-4509-903d-e425b9eb483b): EXPLAIN EXTENDED SELECT COUNT(*) FROM food
INFO  : Starting task [Stage-2:EXPLAIN] in serial mode
INFO  : Completed executing command(queryId=hive_20171123175050_1043dcd7-7560-4509-903d-e425b9eb483b); Time taken: 0.017 seconds
INFO  : OK
+----------------------------------------------------+--+
|                      Explain                       |
+----------------------------------------------------+--+
| ABSTRACT SYNTAX TREE:                              |
|                                                    |
| TOK_QUERY                                          |
|    TOK_FROM                                        |
|       TOK_TABREF                                   |
|          TOK_TABNAME                               |
|             food                                   |
|    TOK_INSERT                                      |
|       TOK_DESTINATION                              |
|          TOK_DIR                                   |
|             TOK_TMP_FILE                           |
|       TOK_SELECT                                   |
|          TOK_SELEXPR                               |
|             TOK_FUNCTIONSTAR                       |
|                COUNT                               |
|                                                    |
|                                                    |
| STAGE DEPENDENCIES:                                |
|   Stage-1 is a root stage                          |
|   Stage-0 depends on stages: Stage-1               |
|                                                    |
| STAGE PLANS:                                       |
|   Stage: Stage-1                                   |
|     Spark                                          |
|       Edges:                                       |
|         Reducer 2 <- Map 1 (GROUP, 1)              |
|       DagName: hive_20171123175050_1043dcd7-7560-4509-903d-e425b9eb483b:3 |
|       Vertices:                                    |
|         Map 1                                      |
|             Map Operator Tree:                     |
|                 TableScan                          |
|                   alias: food                      |
|                   Statistics: Num rows: -1 Data size: 23388 Basic stats: PARTIAL Column stats: COMPLETE |
|                   GatherStats: false               |
|                   Select Operator                  |
|                     Statistics: Num rows: -1 Data size: 23388 Basic stats: PARTIAL Column stats: COMPLETE |
|                     Group By Operator              |
|                       aggregations: count()        |
|                       mode: hash                   |
|                       outputColumnNames: _col0     |
|                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                       Reduce Output Operator       |
|                         sort order:                |
|                         Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                         tag: -1                    |
|                         value expressions: _col0 (type: bigint) |
|                         auto parallelism: false    |
|             Path -> Alias:                         |
|               hdfs://BigData-02:8020/user/hive/warehouse/test_etl.db/food [food] |
|             Path -> Partition:                     |
|               hdfs://BigData-02:8020/user/hive/warehouse/test_etl.db/food  |
|                 Partition                          |
|                   base file name: food             |
|                   input format: org.apache.hadoop.mapred.TextInputFormat |
|                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                   properties:                      |
|                     COLUMN_STATS_ACCURATE false    |
|                     bucket_count -1                |
|                     columns id,name,img_list,img_top,spec,kcal,kj,istr,wgt_unit,wgt_unit_id,is_del,date_create,date_update |
|                     columns.comments 'rec:id - primary key','rec:name - food name','rec:item_image - image url when the food is  persented in the food list','rec:top_image - image url when the food is persented in the home page etc','rec:specifications - like: a bowl of, a glass of','rec:kcal - kilo-calorie','rec:kj - kilojoule','rec:instruction - instruction of this food','rec:weight_standard_name','rec:weight_starnard_id','rec:logic_delete - 0/false:not deleted  1/true:deleted','rec:create_date - create date','rec:update_date - update date' |
|                     columns.types int:string:string:string:string:int:int:string:string:int:boolean:string:string |
|                     comment the basic data of meals |
|                     field.delim 	                  |
|                     file.inputformat org.apache.hadoop.mapred.TextInputFormat |
|                     file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                     location hdfs://BigData-02:8020/user/hive/warehouse/test_etl.db/food |
|                     name test_etl.food             |
|                     numFiles 1                     |
|                     numRows -1                     |
|                     rawDataSize -1                 |
|                     serialization.ddl struct food { i32 id, string name, string img_list, string img_top, string spec, i32 kcal, i32 kj, string istr, string wgt_unit, i32 wgt_unit_id, bool is_del, string date_create, string date_update} |
|                     serialization.format 	         |
|                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                     totalSize 23388                |
|                     transient_lastDdlTime 1509156270 |
|                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                                                    |
|                     input format: org.apache.hadoop.mapred.TextInputFormat |
|                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                     properties:                    |
|                       COLUMN_STATS_ACCURATE false  |
|                       bucket_count -1              |
|                       columns id,name,img_list,img_top,spec,kcal,kj,istr,wgt_unit,wgt_unit_id,is_del,date_create,date_update |
|                       columns.comments 'rec:id - primary key','rec:name - food name','rec:item_image - image url when the food is  persented in the food list','rec:top_image - image url when the food is persented in the home page etc','rec:specifications - like: a bowl of, a glass of','rec:kcal - kilo-calorie','rec:kj - kilojoule','rec:instruction - instruction of this food','rec:weight_standard_name','rec:weight_starnard_id','rec:logic_delete - 0/false:not deleted  1/true:deleted','rec:create_date - create date','rec:update_date - update date' |
|                       columns.types int:string:string:string:string:int:int:string:string:int:boolean:string:string |
|                       comment the basic data of meals |
|                       field.delim 	                |
|                       file.inputformat org.apache.hadoop.mapred.TextInputFormat |
|                       file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                       location hdfs://BigData-02:8020/user/hive/warehouse/test_etl.db/food |
|                       name test_etl.food           |
|                       numFiles 1                   |
|                       numRows -1                   |
|                       rawDataSize -1               |
|                       serialization.ddl struct food { i32 id, string name, string img_list, string img_top, string spec, i32 kcal, i32 kj, string istr, string wgt_unit, i32 wgt_unit_id, bool is_del, string date_create, string date_update} |
|                       serialization.format 	       |
|                       serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                       totalSize 23388              |
|                       transient_lastDdlTime 1509156270 |
|                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
+----------------------------------------------------+--+
|                      Explain                       |
+----------------------------------------------------+--+
|                     name: test_etl.food            |
|                   name: test_etl.food              |
|             Truncated Path -> Alias:               |
|               /test_etl.db/food [food]             |
|         Reducer 2                                  |
|             Needs Tagging: false                   |
|             Reduce Operator Tree:                  |
|               Group By Operator                    |
|                 aggregations: count(VALUE._col0)   |
|                 mode: mergepartial                 |
|                 outputColumnNames: _col0           |
|                 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                 File Output Operator               |
|                   compressed: false                |
|                   GlobalTableId: 0                 |
|                   directory: hdfs://BigData-02:8020/tmp/hive/hdfs/f7aa029d-d02a-4b83-a9f0-008a4f7f7dd9/hive_2017-11-23_17-50-54_134_754704724368784822-1/-mr-10000/.hive-staging_hive_2017-11-23_17-50-54_134_754704724368784822-1/-ext-10001 |
|                   NumFilesPerFileSink: 1           |
|                   Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE |
|                   Stats Publishing Key Prefix: hdfs://BigData-02:8020/tmp/hive/hdfs/f7aa029d-d02a-4b83-a9f0-008a4f7f7dd9/hive_2017-11-23_17-50-54_134_754704724368784822-1/-mr-10000/.hive-staging_hive_2017-11-23_17-50-54_134_754704724368784822-1/-ext-10001/ |
|                   table:                           |
|                       input format: org.apache.hadoop.mapred.TextInputFormat |
|                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                       properties:                  |
|                         columns _col0              |
|                         columns.types bigint       |
|                         escape.delim \             |
|                         hive.serialization.extend.additional.nesting.levels true |
|                         serialization.format 1     |
|                         serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                   TotalFiles: 1                    |
|                   GatherStats: false               |
|                   MultiFileSpray: false            |
|                                                    |
|   Stage: Stage-0                                   |
|     Fetch Operator                                 |
|       limit: -1                                    |
|       Processor Tree:                              |
|         ListSink                                   |
|                                                    |
+----------------------------------------------------+--+
140 rows selected (0.242 seconds)

```

## Limit Tuning \Limit 优化 =>在使用limit的时候,不对所有数据进行查询,而是按照一定规则进行抽样

The LIMIT clause is commonly used, often by people working with the CLI. However, i`n many cases a LIMIT clause still executes the entire query`, then only returns a handful of results. Because this behavior is generally wasteful, it should be avoided when possible. Hive has a configuration property to enable sampling of source data for use with LIMIT:

```xml
<property>
 <name>hive.limit.optimize.enable</name>
 <value>true</value>
 <description>Whether to enable to optimization to try a smaller subset of data for simple LIMIT first.</description>
</property>
```

Once the `hive.limit.optimize.enable` is set to true, two variables control its operation, `hive.limit.row.max.siz`e and `hive.limit.optimize.limit.file`:

```xml
<property>
 <name>hive.limit.row.max.size</name>
 <value>100000</value>
 <description>When trying a smaller subset of data for simple LIMIT,
 how much size we need to guarantee each row to have at least.
 </description>
</property>
<property>
 <name>hive.limit.optimize.limit.file</name>
 <value>10</value>
 <description>When trying a smaller subset of data for simple LIMIT,
 maximum number of files we can sample.</description>
</property>
```

A drawback of this feature is the risk that useful input data will never get processed. For example, any query that requires a reduce step, such as most JOIN and GROUP BY operations, most calls to aggregate functions, etc., will have very different results. Perhaps this difference is okay in many cases, but it’s important to understand.\
`这个设置会导致有些数据可能一直访问数据,而且查询产生的结果会出现不同`

## Optimized Joins \ Join 优化 => 在第六章讲解了JOIN

We discussed optimizing join performance in “Join Optimizations” on page 100 and “Map-side Joins” on page 105. We won’t reproduce the details here, but just remind yourself `that it’s important to know which table is the largest and put it last in the JOIN clause, or use the /* streamtable(table_name) */ directive.`

`If all but one table is small enough, typically to fit in memory`, then Hive can perform a map-side join, eliminating the need for reduce tasks and even some map tasks. Sometimes even tables that do not fit in memory are good candidates because removing the reduce phase outweighs the cost of bringing semi-large tables into each map tasks.

- 把大表放在JOIN语句最右边
- 把足够小的表放入内存中

## Local Mode \本地模式 =>对于数据较少的情况,可以用一台机器甚至单个进程来执行这个job

Many Hadoop jobs need the full scalability benefits of Hadoop to process large data sets. However, there are times when the input to Hive is very small. In these cases, the overhead of launching tasks for queries consumes a significant percentage of the overall job execution time. In many of these cases, Hive can leverage the lighter weight of the local mode to `perform all the tasks for the job on a single machine` and sometimes in the same process. The reduction in execution times can be dramatic for small data sets. You can explicitly enable local mode temporarily, as in this example:

```sql
hive> set oldjobtracker=${hiveconf:mapred.job.tracker};
hive> set mapred.job.tracker=local;
hive> set mapred.tmp.dir=/home/edward/tmp;
hive> SELECT * from people WHERE firstname=bob;
...
hive> set mapred.job.tracker=${oldjobtracker};
```

You can also tell Hive to automatically apply this optimization by setting `hive.exec.mode.local.auto` to true, perhaps in your $HOME/.hiverc. To set this property permanently for all users, change the value in your $HIVE_HOME/conf/hive-site.xml:\
`在配置文件中设置自动启动本地模式进行优化`

```xml
<property>
 <name>hive.exec.mode.local.auto</name>
 <value>true</value>
 <description>
 Let hive determine whether to run in local mode automatically
 </description>
</property>
```

## Parallel Execution \ 并行执行

Hive converts a query into one or more stages. Stages could be a MapReduce stage, a sampling stage, a merge stage, a limit stage, or other possible tasks Hive needs to do. By default, Hive executes these stages one at a time. However, a particular job may consist of some stages that are not dependent on each other and could be executed in parallel, possibly allowing the overall job to complete more quickly. However, if more stages are run simultaneously, the job may complete much faster.\
`Hive一个查询可能被分解为许多步骤,但是其中有些步骤没有什么关联是可以被并行执行的`

Setting `hive.exec.parallel` to true enables parallel execution. Be careful in a shared cluster, however. If a job is running more stages in parallel, it will increase its cluster utilization:

```sql
<property>
 <name>hive.exec.parallel</name>
 <value>true</value>
 <description>Whether to execute jobs in parallel</description>
</property>
```

## Strict Mode \ 严格模式

Strict mode is a setting in Hive that prevents users from issuing queries that could have unintended and undesirable effects.

- Setting the property `hive.mapred.mode` to strict disables three types of queries.
  - First, queries on partitioned tables are not permitted unless they include a partition filter in the WHERE clause, limiting their scope. In other words, you’re prevented from queries that will scan all partitions. The rationale for this limitation is that partitioned tables often hold very large data sets that may be growing rapidly. An unrestricted partition could consume unacceptably large resources over such a large table:\
  `分区表必须指定分区`

    ```sql
    hive> SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id=5;
    FAILED: Error in semantic analysis: No Partition Predicate Found for
    Alias "fracture_ins" Table "fracture_ins"
    ```
    The following enhancement adds a partition filter—the table partitions—to the WHERE clause:
    ```sql
    hive> SELECT DISTINCT(planner_id) FROM fracture_ins
     > WHERE planner_id=5 AND hit_date=20120101;
    ... normal results ...
    ```

  - The second type of restricted query are `those with ORDER BY clauses, but no LIMIT clause`. Because ORDER BY sends all results to a single reducer to perform the ordering, forcing the user to specify a LIMIT clause prevents the reducer from executing for an extended period of time:
    ```sql
    hive> SELECT * FROM fracture_ins WHERE hit_date>2012 ORDER BY planner_id;
    FAILED: Error in semantic analysis: line 1:56 In strict mode,
    limit must be specified if ORDER BY is present planner_id
    ```

    To issue this query, add a LIMIT clause:
    ```sql
    hive> SELECT * FROM fracture_ins WHERE hit_date>2012 ORDER BY planner_id
     > LIMIT 100000;
    ... normal results ...
    ```
  - The third and final type of query prevented is a `Cartesian product[笛卡儿积]`. Users coming from the relational database world may expect that queries that perform a JOIN not with an ON clause but with a WHERE clause will have the query optimized by the query planner, effectively converting the WHERE clause into an ON clause. Unfortunately, Hive does not perform this optimization, so a runaway query will occur if the tables are large:\
  `hive不像传统关系数据库那样能够高效的把where语句转变成on语句`
    ```sql
    hive> SELECT * FROM fracture_act JOIN fracture_ads
     > WHERE fracture_act.planner_id = fracture_ads.planner_id;
    FAILED: Error in semantic analysis: In strict mode, cartesian product
    is not allowed. If you really want to perform the operation,
    +set hive.mapred.mode=nonstrict+
    ```

    Here is a properly constructed query with JOIN and ON clauses:
    ```sql
    hive> SELECT * FROM fracture_act JOIN fracture_ads
     > ON (fracture_act.planner_id = fracture_ads.planner_id);
    ... normal results ...
    ```

## Tuning the Number of Mappers and Reducers \ 优化Mapper和Reducer的数量

Hive is able to parallelize queries by breaking the query into one or more MapReduce jobs. Each of which might have multiple mapper and reducer tasks, at least some of which can run in parallel. Determining the optimal number of mappers and reducers depends on many variables, such as the size of the input and the operation being performed on the data.

A balance is required. Having too many mapper or reducer tasks causes excessive overhead in starting, scheduling, and running the job, while too few tasks means the inherent parallelism of the cluster is underutilized.

When running a Hive query that has a reduce phase, the CLI prints information about how the number of reducers can be tuned. Let’s see an example that uses a GROUP BY query, because they always require a reduce phase. In contrast, many other queries are converted into map-only jobs:

```sql
hive> SELECT pixel_id, count FROM fracture_ins WHERE hit_date=20120119
 > GROUP BY pixel_id;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
 set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
 set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
 set mapred.reduce.tasks=<number>
...
```

Hive is determining the number of reducers from the input size. This can be confirmed using the dfs -count command, which works something like the Linux du -s command; it computes a total size for all the data under a given directory:

```bash
[edward@etl02 ~]$ hadoop dfs -count /user/media6/fracture/ins/* | tail -4
 1 8 2614608737 hdfs://.../user/media6/fracture/ins/hit_date=20120118
 1 7 2742992546 hdfs://.../user/media6/fracture/ins/hit_date=20120119
 1 17 2656878252 hdfs://.../user/media6/fracture/ins/hit_date=20120120
 1 2 362657644 hdfs://.../user/media6/fracture/ins/hit_date=20120121
```

(We’ve reformatted the output and elided some details for space.)

The default value of `hive.exec.reducers.bytes.per.reducer` is 1 GB. Changing this value to 750 MB causes Hive to estimate four reducers for this job:

```sql
hive> set hive.exec.reducers.bytes.per.reducer=750000000;
hive> SELECT pixel_id,count(1) FROM fracture_ins WHERE hit_date=20120119
 > GROUP BY pixel_id;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 4
...
```

This default typically yields good results. However, there are cases where a query’s map phase will create significantly more data than the input size. In the case of excessive map phase data, the input size of the default might be selecting too few reducers. Likewise the map function might filter a large portion of the data from the data set and then fewer reducers may be justified.\
`有时候map流程可能产生比源数据多得多的数据,这时候reducer按照源数据量来进行分配可能就会显得比较少了`

A quick way to experiment is by setting the number of reducers to a `fixed size`, rather than allowing Hive to calculate the value. If you remember, the `Hive default estimate is three reducers`. Set `mapred.reduce.tasks` to different numbers and determine if more or fewer reducers results in faster run times. Remember that benchmarking like this is complicated by external factors such as other users running jobs simultaneously. Hadoop has a few seconds overhead to start up and schedule map and reduce tasks. When executing performance tests, it’s important to keep these factors in mind, especially if the jobs are small.

The `hive.exec.reducers.max property` is useful for controlling resource utilization on shared clusters when dealing with large jobs. A Hadoop cluster has a fixed number of map and reduce `“slots”` to allocate to tasks. One large job could reserve all of the slots and block other jobs from starting. Setting `hive.exec.reducers.max` can stop a query from taking too many reducer resources. It is a good idea to set this value in your $HIVE_HOME/conf/hive-site.xml. A suggested formula is to set the value to the result of this calculation:

`(Total Cluster Reduce Slots * 1.5) / (avg number of queries running)`

The 1.5 multiplier is a fudge factor to prevent underutilization of the cluster.

## JVM Reuse \ JVM重用

`JVM reuse is a Hadoop tuning parameter` that is very relevant to Hive performance, especially `scenarios` where it’s hard to avoid small files and scenarios with lots of tasks, most which have short execution times.\
`对于那些不得不处理很多小文件,或者产生很多task的情况,JVM重用非常重要`

The default configuration of Hadoop will typically launch map or reduce tasks in a forked JVM. The JVM start-up may create significant overhead, especially when launching jobs with hundreds or thousands of tasks. Reuse allows a JVM instance to be reused up to N times for the same job. This value is set in Hadoop’s `mapredsite.xml`

(in $HADOOP_HOME/conf):

```xml
<property>
 <name>mapred.job.reuse.jvm.num.tasks</name>
 <value>10</value>
 <description>How many tasks to run per jvm. If set to -1, there is no limit.
 </description>
</property>
```

A drawback of this feature is that JVM reuse will keep reserved task slots open until the job completes, in case they are needed for reuse. If an `“unbalanced”` job has some reduce tasks that run considerably longer than the others, the reserved slots will sit idle, unavailable for other jobs, until the last task completes.\
`带来的副作用就是JVM重用的时候,会占用相应数量的slots(task名额)`

## Indexes

Indexes may be used to accelerate the calculation speed of a GROUP BY query. Hive contains an implementation of bitmap indexes since v0.8.0. The main use case for bitmap indexes is when there are comparatively few values for a given column. See “Bitmap Indexes” on page 118 for more information.\`索引主要在第八章`

## Dynamic Partition Tuning \ 动态分区

As explained in “Dynamic Partition Inserts” on page 74[第五章], dynamic partition INSERT statements enable a succinct SELECT statement to create many new partitions for insertion into a partitioned table.

This is a very powerful feature, however if the number of partitions is high, a large number of output handles must be created on the system. This is a somewhat uncommon use case for Hadoop, which typically creates a few files at once and streams large amounts of data to them.

Out of the box, Hive is configured to prevent dynamic partition inserts from creating more than 1,000 or so partitions. While it can be bad for a table to have too many partitions, it is generally better to tune this setting to the larger value and allow these queries to work.

First, it is always good to set the dynamic partition mode to strict in your hivesite.xml,as discussed in “Strict Mode” on page 137. When strict mode is on, at least one partition has to be static, as demonstrated in “Dynamic Partition Inserts” on page 74:

```xml
<property>
 <name>hive.exec.dynamic.partition.mode</name>
 <value>strict</value>
 <description>In strict mode, the user must specify at least one
static partition in case the user accidentally overwrites all
partitions.</description>
</property>
Then, increase the other relevant properties to allow queries that will create a large
number of dynamic partitions, for example:
<property>
 <name>hive.exec.max.dynamic.partitions</name>
 <value>300000</value>
 <description>Maximum number of dynamic partitions allowed to be
created in total.</description>
</property>
<property>
 <name>hive.exec.max.dynamic.partitions.pernode</name>
 <value>10000</value>
 <description>Maximum number of dynamic partitions allowed to be
created in each mapper/reducer node.</description>
</property>
```

Another setting controls how many files a DataNode will `allow to be open at once`. It must be set in the DataNode’s $HADOOP_HOME/conf/hdfs-site.xml.

In Hadoop v0.20.2, the default value is 256, which is too low. The value affects the number of maximum threads and resources, so setting it to a very high number is not recommended. Note also that in Hadoop v0.20.2, changing this variable requires restarting the DataNode to take effect:

```xml
<property>
 <name>dfs.datanode.max.xcievers</name>
 <value>8192</value>
</property>
```

## Speculative Execution \ 推测执行

Speculative execution is a feature of Hadoop that launches a certain number of duplicate tasks. While this consumes more resources computing duplicate copies of data that may be discarded, the goal of this feature is to improve overall job progress by getting individual task results faster, and detecting then black-listing slow-running TaskTrackers.\
`原来hadoop在分配完map reduce task后，会预测性的判断某个map 或reduce task所在的节点资源有限，执行会比较慢，因此他在资源更多的节点上会启动一个完全一样的map或 reduce task，同时执行，哪个先完成，就将未完成的那个task kill掉，提高整体job效率。`

Hadoop speculative execution is controlled in the $HADOOP_HOME/conf/mapredsite.xml file by the following two variables:

```xml
<property>
 <name>mapred.map.tasks.speculative.execution</name>
 <value>true</value>
 <description>If true, then multiple instances of some map tasks
 may be executed in parallel.</description>
</property>
<property>
 <name>mapred.reduce.tasks.speculative.execution</name>
 <value>true</value>
 <description>If true, then multiple instances of some reduce tasks
 may be executed in parallel.</description>
</property>
However, Hive provides its own variable to control reduce-side speculative execution:
Speculative Execution | 141
<property>
 <name>hive.mapred.reduce.tasks.speculative.execution</name>
 <value>true</value>
 <description>Whether speculative execution for
 reducers should be turned on. </description>
</property>
```

It is hard to give a concrete recommendation about tuning these speculative execution variables. If you are very sensitive to deviations in runtime, you may wish to turn these features on. However, if you have long-running map or reduce tasks due to large amounts of input, the waste could be significant.

##　Single MapReduce MultiGROUP BY

Another special optimization attempts to combine multiple GROUP BY operations in a query into a single MapReduce job. For this optimization to work, a common set of GROUP BY keys is required:

```xml
<property>
 <name>hive.multigroupby.singlemr</name>
 <value>false</value>
 <description>Whether to optimize multi group by query to generate single M/R
 job plan. If the multi group by query has common group by keys, it will be
 optimized to generate single M/R job.</description>
</property>
```

｀如果多个group by查询,有同样的group key,会被优化到同一个mr job中｀

## Virtual Columns / 虚拟列

- INPUT__FILE__NAME : 输入文件名
- BLOCK__OFFSET__INSIDE__FILE : 块内偏移量
- ROW__OFFSET__INSIDE__BLOCK : 行偏移量 需要设置

Hive provides `two virtual columns`: `one for the input filename for split` and `the other for the block offset in the file`. These are helpful when diagnosing queries where Hive is producing unexpected or null results. By projecting these “columns,” you can see which file and row is causing problems:\
`我的理解:hive把不能解析的内容都解析成null,这个设计可以帮助定位出问题的数据,这个似乎是错的,应该是错的,大概率是错的.另外有一个说法:虚拟列：虚拟列是Hive中特殊的列的特殊函数类型。目前为止Hive仅支持2个虚拟列：INPUT_FILE_NAME和BLOCK_OFFSET_INSIDE_FILE。INPUT_FILE_NAME列是mapper的输入文件名，BLOCK_OFFSET_INSIDE_FILE是当前全部文件位置或当前压缩文件的块偏移量。=>看着个说法,大概是一个query会设计一个物理表下面的几个文件(表是个文件夹,里面有好几文件),这样能定位出事的数据,是在哪一个文件里面的`
`在另外一个地方看到虚拟列 是 分区 这个说法可以理解,不过一定不是现在这个 虚拟列`

```sql
hive> set hive.exec.rowoffset=true;
hive> SELECT INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE, line
 > FROM hive_text WHERE line LIKE '%hive%' LIMIT 2;

har://file/user/hive/warehouse/hive_text/folder=docs/
data.har/user/hive/warehouse/hive_text/folder=docs/README.txt 2243
 http://hive.apache.org/
har://file/user/hive/warehouse/hive_text/folder=docs/
data.har/user/hive/warehouse/hive_text/folder=docs/README.txt 3646
- Hive 0.8.0 ignores the hive-default.xml file, though we continue
```

(We wrapped the long output and put a blank line between the two output rows.)

A third virtual column provides the row offset of the file. It must be enabled explicitly:

```xml
<property>
 <name>hive.exec.rowoffset</name>
 <value>true</value>
 <description>Whether to provide the row offset virtual column</description>
</property>
```

Now it can be used in queries:
hive> SELECT INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE,
 > ROW__OFFSET__INSIDE__BLOCK
 > FROM hive_text WHERE line LIKE '%hive%' limit 2;
file:/user/hive/warehouse/hive_text/folder=docs/README.txt 2243 0
file:/user/hive/warehouse/hive_text/folder=docs/README.txt 3646 0